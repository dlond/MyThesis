%!TEX root = /Users/dan/Documents/Thesis/Thesis.tex
% Chapter 5

\chapter{1-Cohomology Calculation}
\label{Chapter5}
\lhead{Chapter 5. \emph{1-Cohomology Calculation}}

In this chapter we present a method of calculating the 1-cohomology $H^1\left( SL_2(k), V\right)$ where $V=R_u(P)$ is the unipotent radical of a parabolic subgroup $P$ of a reductive group $G$. The motivation for this is to look for infinitely many conjugacy classes of representations of $SL_2(k)$ into $G$ in the hope of finding a finite subgroup $H$ of $SL_2(k)$ as a counterexample for K\"ulshammer's Second Problem.

\section{The method}

Let $G$ be a reductive group over an algebraically closed field $k$ of characteristic $p$. Let $\Phi$ be the roots for $G$ with $\Delta \subset \Phi^+ \subset \Phi$ the simple and positive roots, respectively, associated to a fixed maximal torus $T$ of $G$. 

[I want to see if this works for arbitrary rank] Let $P_\alpha<G$ be the parabolic subgroup of $G$ corresponding to the simple root $\alpha\in\Delta$, with Levi subgroup $L_\alpha$ and unipotent radical $V_\alpha$:
\begin{eqnarray*}
V_\alpha=R_u(P_\alpha) &=& \langle U_\delta \in \Phi^+\, | \, \delta \neq \alpha \rangle,\\
P_\alpha &=& L_\alpha \ltimes V_\alpha.
\end{eqnarray*}

By [reference] there exists a homomorphism $\rho_0$ from $ SL_2(k)$ into $L_\alpha$ under which
\begin{eqnarray*}
\rho_0 \left(\begin{matrix} 1 &  u \\ 0 & 1 \end{matrix} \right) &=& \epsilon_\alpha(u) \\
\rho_0 \left(\begin{matrix} 1 & 0 \\ u & 1 \end{matrix} \right) &=& \epsilon_{-\alpha}(u)
\end{eqnarray*}

We fix an integer $r > 0$ and define $\rho_r$ to be the homomorphism from $SL_2(k)$ into $L_\alpha$ composed of $\rho_0$ and the Frobenius map,
\begin{eqnarray*}
F_r&:&SL_2(k)\rightarrow SL_2(k) \\
&& (A_{ij}) \mapsto (A_{ij})^{p^r}.
\end{eqnarray*}
That is
\begin{eqnarray*}
\rho_r &=& \rho_0 \circ F_r,
\end{eqnarray*}
and satisfies
\begin{eqnarray*}
\rho_r \left(\begin{matrix} 1 &  u \\ 0 & 1 \end{matrix} \right) &=& \epsilon_\alpha(u^{p^r}) \\
\rho_r \left(\begin{matrix} 1 & 0 \\ u & 1 \end{matrix} \right) &=& \epsilon_{-\alpha}(u^{p^r}).
\end{eqnarray*}

We let $SL_2(k)$ act on $V_\alpha$ via $\rho_r$ and we consider 1-cocycles $\sigma \in Z^1(SL_2(k), V_\alpha)$. As we are interested in 1-cohomology classes, we may as well only consider those 1-cocycles that are zero on a maximal torus of $SL_2(k)$ [reference], so let $\sigma \in Z^1(SL_2(k), V_\alpha)$ such that
\begin{eqnarray*}
\sigma\left(\left(\begin{matrix} t & 0 \\ 0 & t^{-1}\end{matrix}\right)\right) = 0,
\end{eqnarray*}
for all $t\in k^*$.
We can say a few things about these particular 1-cocycles which help us calculate the 1-cohomology. We refer to the results in [reference]:
\begin{eqnarray*}
\left(\begin{matrix} t & 0 \\ 0 & t^{-1}\end{matrix}\right) \cdot \prod_\delta \epsilon_\delta (\lambda_\delta) &=&
\prod_\delta \epsilon_\delta\left( (t^{p^r})^{\langle \delta, \alpha\rangle}\lambda_\delta\right) \\
\left(\begin{matrix} 0 & -1 \\ 1 & 0 \end{matrix}\right) \cdot \prod_\delta \epsilon_\delta (\lambda_\delta) &=&
\prod_\delta n_\alpha \epsilon_\delta\left( \lambda_\delta\right)\, n_\alpha^{-1},
\end{eqnarray*}
where $n_\alpha = \epsilon_\alpha(1)\epsilon_{-\alpha}(-1)\epsilon_\alpha(1)$ and $\lambda_\delta$ are elements of the underlying field $k$.

\begin{lemma} \label{claim1}
\begin{eqnarray*}
\sigma\left(\left(\begin{matrix} 1 & u \\ 0 & 1 \end{matrix}\right)\right) = \prod_\delta\, \epsilon_\delta\left(x_\delta\left(u\right)\right),
\end{eqnarray*}
where $\delta$ ranges $\Phi^+ - \{\alpha\}$ such that $\langle \delta, \alpha \rangle > 0$, and $x_\delta\in k[T]$ are polynomials in one variable.
\end{lemma}
\begin{proof}
We have the chain of morphisms
\begin{eqnarray*}
k\cong \left(\begin{matrix}1 & * \\ 0 & 1\end{matrix}\right) 
\stackrel{i}\longrightarrow SL_2(k) 
\stackrel{\sigma}\longrightarrow V_\alpha 
\stackrel{\pi_\delta}\longrightarrow k
\end{eqnarray*}
where $i$ is the inclusion map and $\pi_\delta$ the projection onto the root subgroup $V_\delta$. Hence, by the definition
\begin{eqnarray*}
x_\delta = \pi_\delta\, \circ\, \sigma\, \circ\, i
\end{eqnarray*}
is a morphism from $k \rightarrow k$.

Now since
\begin{eqnarray*}
\left(\begin{matrix}
t & 0 \\ 0 & t^{-1}
\end{matrix}\right)
\left(\begin{matrix}
1 & u \\ 0 & 1
\end{matrix}\right)
\left(\begin{matrix}
t^{-1} & 0 \\ 0 & t
\end{matrix}\right)
=
\left(\begin{matrix}
1 & t^2u \\ 0 & 1
\end{matrix}\right),
\end{eqnarray*}

we use the 1-cocycle condition to obtain
\begin{eqnarray*}
\sigma\left(
\left(\begin{matrix}
1 & t^2u \\ 0 & 1
\end{matrix}\right)
\right)
&=&\sigma\left(
\left(\begin{matrix}
t & 0 \\ 0 & t^{-1}
\end{matrix}\right)
\left(\begin{matrix}
1 & u \\ 0 & 1
\end{matrix}\right)
\left(\begin{matrix}
t^{-1} & 0 \\ 0 & t
\end{matrix}\right)
\right)\\
&=&
\sigma\left(
\left(\begin{matrix}
t & 0 \\ 0 & t^{-1}
\end{matrix}\right)
\right)
\left(\begin{matrix}
t & 0 \\ 0 & t^{-1}
\end{matrix}\right) \cdot
\sigma\left(
\left(\begin{matrix}
1 & u \\ 0 & 1
\end{matrix}\right)
\left(\begin{matrix}
t^{-1} & 0 \\ 0 & t
\end{matrix}\right)
\right)\\
&=&
\sigma\left(
\left(\begin{matrix}
t & 0 \\ 0 & t^{-1}
\end{matrix}\right)
\right)
\left(\begin{matrix}
t & 0 \\ 0 & t^{-1}
\end{matrix}\right) \cdot
\sigma\left(
\left(\begin{matrix}
1 & u \\ 0 & 1
\end{matrix}\right)
\right)
\left(\begin{matrix}
1 & u \\ 0 & 1
\end{matrix}\right) \cdot
\sigma\left(
\left(\begin{matrix}
t^{-1} & 0 \\ 0 & t
\end{matrix}\right)
\right)\\
&=&
\left(\begin{matrix}
t & 0 \\ 0 & t^{-1}
\end{matrix}\right) \cdot
\sigma\left(
\left(\begin{matrix}
1 & u \\ 0 & 1
\end{matrix}\right)
\right).
\end{eqnarray*}
Therefore
\begin{eqnarray*}
x_\delta\left(t^2u\right) &=& (t^{p^r})^{\langle \delta, \alpha\rangle}x_\delta\left(u\right).
\end{eqnarray*}
Since $x_\delta$ is a polynomial function there can only be non-negative powers of $t$ on the left-hand side of the equality which forces $\langle \delta, \alpha \rangle \geq 0$. However, if $\langle \delta, \alpha \rangle = 0$ then $x_\delta$ is constant and hence zero, as $\sigma$ is zero on $\left(\begin{matrix} * & 0 \\ 0 & *\end{matrix}\right)$. Therefore the non-zero $x_\delta$ occur precisely when $\langle \delta, \alpha \rangle > 0$.
\end{proof}

%\begin{definition} Suppose $\alpha, \beta \in \Phi$ are linearly independent. Since $\Phi$ is finite there exist integers $p,q$ such that $i\alpha + \beta \in \Phi$ for $-p\leq i \leq q$ but $-(p+1)\alpha + \beta$ and $(q+1)\alpha + \beta$ are not in $\Phi$. The sequence of roots
%\begin{eqnarray*}
%-p\alpha + \beta,\ldots, \beta, \ldots, q\alpha + \beta
%\end{eqnarray*}
%is called the $\alpha$-chain of roots through $\beta$.
%\end{definition}

%\begin{lemma} \label{chainpair}
%$\langle \alpha, \beta \rangle = p - q.$
%\end{lemma}
%\begin{proof} See [reference]\end{proof}

Next we prove a couple of useful facts about root systems not containing $G_2$ or $C_3$.

\begin{lemma} \label{ufixes} Suppose $\Phi$ is not of type $G_2$ and let $\alpha,\beta\in\Phi$. If $\alpha + \beta \in \Phi$ then $\langle \alpha, \beta \rangle \leq 0$.
\end{lemma}
\begin{proof}
\begin{eqnarray*}
\langle \alpha, \beta \rangle > 0 \Longleftrightarrow (\alpha, \beta) >0 \Longleftrightarrow \cos(\theta) > 0,
\end{eqnarray*}
where $\theta$ is the angle between $\alpha$ and $\beta$. Hence acute angles correspond to positive pairs. Referring to the $A_2$ and $B_2$ root system diagrams we find that no two roots meeting at an acute angle add to give another root. Therefore if $\langle \alpha, \beta \rangle > 0$ then $\alpha + \beta \notin \Phi$.
\end{proof}

We must exclude the case $\Phi = G_2$ here since $\alpha, 2\alpha + \beta$ and $3\alpha + \beta$ are all roots ($\alpha$ short) but $\langle \alpha, 2\alpha + \beta \rangle = 1$. %Since the following results depend upon \ref{ufixes} we will henceforth assume that $\Phi \neq G_2$ and deal with the case $\Phi = G_2$ by way of an example.

\begin{lemma} \label{uabelian}
Suppose $\Phi$ does not contain $G_2$ or $C_3$. Let $\delta_1, \delta_2 \in \Phi$ and $\gamma \in \Delta$ be roots such that $\langle \delta_i, \gamma \rangle > 0$ $(i = 1, 2)$. If $\delta_1 + \delta_2$ is a root, then $\delta_1$ and $\delta_2$ are of opposite sign.
\end{lemma}
\begin{proof}
Suppose $\delta_1 + \delta_2 \in \Phi$. Let $\theta_i$ be the absolute value of the angle between $\delta_i$ and $\gamma$, $(i = 1,2)$ and let $\theta_3$ be the absolute value of the angle between $\delta_1$ and $\delta_2$. Then
\begin{eqnarray*}
&& \langle \delta_i, \gamma\rangle > 0 \qquad (i=1,2) \\
& \Longrightarrow& (\delta_i, \gamma) > 0 \\
& \Longrightarrow& \cos(\theta_i) > 0 \\
& \Longrightarrow& \theta_i < \pi/2,
\end{eqnarray*}
and similarly, using \ref{ufixes}
\begin{eqnarray*}
&& \langle \delta_1, \delta_2 \rangle \leq 0 \\
& \Longrightarrow& \theta_3 \geq \pi/2.
\end{eqnarray*}
So, without loss of generality, this leads to consider four cases:
\begin{eqnarray*}
\textbf{1:}&&\theta_1 = \pi/3,\quad\theta_2 = \pi/3,\quad\theta_3 = 2\pi/3; \\
\textbf{2:}&&\theta_1 = \pi/3,\quad\theta_2 = \pi/3,\quad\theta_3 = \pi/2; \\
\textbf{3:}&&\theta_1 = \pi/4,\quad\theta_2 = \pi/3,\quad\theta_3 = \pi/2; \\
\textbf{4:}&&\theta_1 = \pi/4,\quad\theta_2 = \pi/4,\quad\theta_3 = \pi/2.
\end{eqnarray*}
[Wow, probably need more explanation there] 

For the cases in which $\theta_3 = \pi/2$ we can reason from the root system diagrams that $\delta_1$ and $\delta_2$ lie in a $B_2$ subsystem of $\Phi$, and they have the same length. Since $\delta_1+\delta_2$ is a root it must be that $\delta_1$ and $\delta_2$ are short roots and their sum is a long root. However we must rule out the third case. For if $\theta_1 = \pi/4$ then $\delta_1$ and $\gamma$ are roots of different length in a $B_2$ subsystem, but $\theta_2 = \pi/3$ implies that $\delta_2$ and $\gamma$ are roots of the same length in an $A_2$ subsystem, which is absurd.

The three roots must lie in a plane for cases one and four. That is they lie in some rank 2 subsystem; $A_2$ and $B_2$ respectively. Consulting the root system diagrams yields $\gamma = \delta_1 + \delta_2$ and the result holds.

In the second case we see that $\delta_1, \delta_2$ and $\gamma$ do not lie together in a rank 2 subsystem, and that these roots are the same length which implies that $\gamma$ is a short root. In fact, since a pair short roots lie in subsystems of type $A_2$ it must be that the rank 3 subsystem in which the four roots lie is of type $C_3$. [Picture?][Wow, is that right? Maybe just say `we will show that they lie in a $C_3$ subsystem'.]
% Let $\theta_4$ be the absolute value of the angle between $\delta_1+\delta_2$ and $\gamma$. Then
% \begin{eqnarray*}
% \cos(\theta_4) &=& \frac{(\delta_1 + \delta_2, \gamma)}{|\delta_1+ \delta_2||\gamma|}\\
% &>&\frac{(\delta_1, \gamma)}{2|\delta_1||\gamma|} + \frac{(\delta_2, \gamma)}{2|\delta_2||\gamma|}\quad\textrm{since }|\delta_1+\delta_2| < |\delta_1| + |\delta_2|\\
% &=& \cos(\pi/3).
% \end{eqnarray*}
% Hence $\theta_4<\pi/3$. The only possibility is for $\theta_4 = \pi/4$ which means that $\delta_1+\delta_2$ is a long root adjacent to $\gamma$ in a $B_2$ subsystem. 
\end{proof}

We return to the 1-cohomology calculation but assume that $G$ does not contain $G_2$ or $C_3$.

\begin{corollary}\label{uact} For any $u_1, u_2 \in k$
\begin{eqnarray*}
\left(\begin{matrix}1 & u_1 \\ 0 & 1 \end{matrix}\right)
\cdot
\sigma\left(\left(\begin{matrix} 1 & u_2 \\ 0 & 1\end{matrix}\right)\right)
=
\sigma\left(\left(\begin{matrix} 1 & u_2 \\ 0 & 1\end{matrix}\right)\right).
\end{eqnarray*}
Furthermore, the $x_\delta$ are homomorphisms.
\end{corollary}

\begin{proof}
We have
\begin{eqnarray*}
\left(\begin{matrix}1 & u_1 \\ 0 & 1 \end{matrix}\right)
\cdot
\sigma\left(\left(\begin{matrix} 1 & u_2 \\ 0 & 1\end{matrix}\right)\right)
&=&
\epsilon_\alpha(u_1^{p^r}) \prod_\delta \epsilon_\delta\left(x_\delta\left(u_2\right)\right) \epsilon_\alpha(-u_1^{p^r}),
\end{eqnarray*}
with $\langle \delta, \alpha \rangle > 0$. By \ref{ufixes} $\alpha + \delta \notin \Phi$ so each $\epsilon_\delta$ commutes with the $\epsilon_\alpha$.
\end{proof}

\begin{corollary} The image of the group of upper triangular matrices of $SL_2(k)$ under $\sigma$ lies in a product of commuting root groups of $V_\alpha$.
\end{corollary}
\begin{proof}
First consider
\begin{eqnarray*}
\sigma\left(\left( \begin{matrix} 1 & b \\ 0 & 1 \end{matrix}\right)\right) &=& \prod_\delta \epsilon_\delta\left(x_\delta(b)\right).
\end{eqnarray*}
Suppose the roots $\delta_1$ and $\delta_2$ appear on the right hand side. By \ref{claim1} $\delta_i \in \Phi^+ - \{\alpha\}$ and $\langle \delta_i, \alpha \rangle > 0$ $(i=1,2)$, so \ref{uabelian} asserts that $\delta_1 + \delta_2$ is no root, hence, $\epsilon_{\delta_1}$ and $\epsilon_{\delta_2}$ commute. 

Therefore, for any $a, b\in k$ with $a\neq 0$
\begin{eqnarray*}
\sigma\left(\left(\begin{matrix} a & ab \\ 0 & a^{-1}\end{matrix}\right)\right) 
&=& \left(\begin{matrix} a & 0 \\ 0 & a^{-1}\end{matrix} \right) \cdot
\sigma\left(\left(\begin{matrix} 1 & b \\ 0 & 1\end{matrix}\right)\right) \\
&=& \prod_\delta \epsilon_\delta\left(a^{\langle \delta, \alpha \rangle p^r}x_\delta\left(b\right)\right).
\end{eqnarray*}
\end{proof}

Since the $x_\delta$ are homomorphisms from $k\rightarrow k$ they must take the form
\begin{eqnarray*}
T\mapsto\sum_i \mu_i T^{p^i},
\end{eqnarray*}
for some $\mu_i$ in $k$. Furthermore, combining the calculation in the proof of \ref{claim1} with the result \ref{uact} we get that
\begin{eqnarray*}
\prod_\delta \epsilon_\delta\left(x_\delta\left(a^2b\right)\right) = \prod_\delta \epsilon_\delta\left(a^{\langle \delta, \alpha \rangle p^r}x_\delta\left(b\right)\right),
\end{eqnarray*}
severely restricting the possible polynomials $x_\delta$. In fact, they are confined to be polynomials involving just one term, and the degree has already been decided upon fixing the integer $r$ in the definition of $\rho_r$. For suppose $x_\delta$ and hence some $\mu_j$ is non-zero. Then equating the coefficients of $b$ in the equality directly above yields
\begin{eqnarray*}
\mu_ja^{2p^j} &=& \mu_j a^{\langle \delta, \alpha \rangle p^r}\\
\Longrightarrow2p^j &=& \langle \delta, \alpha \rangle p^r.
\end{eqnarray*}

In [Carter] it is shown that the possible pairings of any two roots are bounded by $\pm 3$. Hence by \ref{claim1} $\langle \delta, \alpha \rangle = 1, 2$ or 3. It is now clear that if $\langle \delta, \alpha \rangle = 3$ then $x_\delta = 0$.

If $\langle \delta, \alpha \rangle = 1$ the characteristic of $k$ must be 2 and $j = r-1$. Otherwise $\langle \delta, \alpha \rangle = 2$ and $j = r$, but the characteristic of $k$ is so far unrestricted.

\begin{example} Let $G=G_2$. Fix a maximal torus, labeling the positive simple roots $\Delta=\{\alpha, \beta\}$ with $\beta$ being the long root. Let 
	\begin{displaymath}
		V_\alpha = R_u(P_\alpha) = \langle U_\beta, U_{\alpha+\beta}, U_{2\alpha+\beta}, U_{3\alpha+\beta}, U_{3\alpha+2\beta}\rangle.
	\end{displaymath}
	We will write $v$ in $V_\alpha$ in angled brackets for compactness:
	\begin{eqnarray*}
		\langle 
		v_1,
		v_2,
		v_3,
		v_4,
		v_5
		\rangle &:=&
		\epsilon_{\beta}(v_1)
		\epsilon_{\alpha+\beta}(v_2)
		\epsilon_{2\alpha+\beta}(v_3)
		\epsilon_{3\alpha+\beta}(v_4)
		\epsilon_{3\alpha+2\beta}(v_5) \in V_\alpha
	\end{eqnarray*}

	The group law for $V_\alpha$ is
	\begin{eqnarray*}
		u * v &=&
		\langle
		u_1 + v_1,
		u_2 + v_2,
		u_3 + v_3,
		u_4 + v_4,
		u_5 + v_5 + 3u_3v_2 - u_4v_1,
		\rangle.
	\end{eqnarray*}
		We compute the action
	\begin{eqnarray*}
		\left(\begin{matrix} a & 0 \\ 0 & a^{-1} \end{matrix}\right)\cdot v &=&
		\langle 
		a^{-3p^r}v_1,
		a^{-p^r}v_2,
		a^{p^r}v_3,
		a^{3p^r}v_4,
		v_5
		\rangle.
	\end{eqnarray*}
	Let $\sigma$ be in $Z^1(SL_2, V_\alpha)$ such that
	\begin{displaymath}
		\sigma\left(\left(\begin{matrix}a & 0\\0 & a^{-1}\end{matrix}\right)\right) = 0.
	\end{displaymath}
	By \ref{claim1}
	\begin{displaymath}
		\sigma\left(\left(\begin{matrix} 1 & b \\ 0 & 1 \end{matrix}\right)\right) =
		\langle 
		0,
		0,
		x_3(b),
		x_4(b),
		0
		\rangle.
	\end{displaymath}
	Applying $\sigma$ to both sides of the identity
	\begin{displaymath}
		\left(\begin{matrix} 1 & a^2b \\ 0 & 1 \end{matrix}\right)
		= 
		\left(\begin{matrix} a & 0 \\ 0 & a^{-1} \end{matrix}\right)
		\left(\begin{matrix} 1 & b \\ 0 & 1 \end{matrix}\right)
		\left(\begin{matrix} a^{-1} & 0 \\ 0 & a \end{matrix}\right),
	\end{displaymath}
	yields
	\begin{eqnarray*}
		x_3(a^2b) &=& a^{p^r}x_3(b)\\
		x_4(a^2b) &=& a^{3p^r}x_4(b).
	\end{eqnarray*}
	Applying $\sigma$ to both sides of the identity
	\begin{eqnarray*}
		\left(\begin{matrix} 1 & b_1 + b_2 \\ 0 & 1\end{matrix}\right) &=&
		\left(\begin{matrix} 1 & b_1 \\ 0 & 1\end{matrix}\right)
		\left(\begin{matrix} 1 & b_2 \\ 0 & 1\end{matrix}\right)
	\end{eqnarray*}
	yields
	\begin{eqnarray*}
		x_3(b_1 + b_2) &=& x_3(b_1) + x_3(b_2)\\
		x_4(b_1 + b_2) &=& x_4(b_1) + x_4(b_2) - 3b_1^{p^r}x_3(b_2).
	\end{eqnarray*}
	We see that $x_3$ is a homomorphism, so it is of the form
	\begin{eqnarray*}
		x_3(b) &=& \sum_i \mu_i b^{p^i}.
	\end{eqnarray*}
	Suppose $x_3\neq 0$. Then some $\mu_j\neq 0$ and
	\begin{eqnarray*}
		&&\mu_j (a^2b)^{p^j} = a^{p^r}\mu_j b^{p^j}\\
		&\Longrightarrow& a^{2p^j} = a^{p^r}\\
		&\Longrightarrow& p = 2.
	\end{eqnarray*}
	But then
	\begin{eqnarray*}
		x_4(0) = x_4(b+b) &=& x_4(b)+x_4(b)-3b^{2^r}x_3(b)\\
		&=& b^{2^r}x_3(b),
	\end{eqnarray*}
	implies that $x_3$ is constant, hence zero. 
	
	Therefore $x_3 = 0$, so $x_4$ is a homomorphism:
	\begin{eqnarray*}
		x_4(b) = \sum_i \nu_i b^{p^r}.
	\end{eqnarray*}
	If $x_4\neq 0$ then there is a $\nu_j\neq 0$ and we get
	\begin{eqnarray*}
		&&\nu_j(a^2b)^{p^j} = a^{3p^r}\nu_j b^{p^j}\\
		&\Longrightarrow& a^{2p^j} = a^{3p^r}\\
		&\Longrightarrow& 2p^j = 3p^r,
	\end{eqnarray*}
	which implies that 2 divides $p$ and 3 divides $p$, a contradiction. Hence $x_4=0$ and
	\begin{displaymath}
		\sigma\left(\left(\begin{matrix}1 & b\\0 & 1\end{matrix}\right)\right) = 0.
	\end{displaymath}
\end{example}

\begin{example} Let $G=C_3$. Fix a maximal torus, labeling the positive simple roots $\Delta=\{\alpha, \beta, \gamma\}$ with $\gamma$ being the long root and connected to $\beta$. Let 
	\begin{displaymath}
		V_\alpha = R_u(P_\alpha) = \langle U_\beta, U_\gamma, U_{\alpha+\beta}, U_{\beta+\gamma}, U_{\alpha+\beta+\gamma}, U_{2\beta+\gamma}, U_{\alpha+2\beta+\gamma}, U_{2\alpha+2\beta+\gamma}\rangle.
	\end{displaymath}
	Again we will write $v$ in $V_\alpha$ in angled brackets for ease of notation:
	\begin{eqnarray*}
		&&\langle 
		v_1,
		v_2,
		v_3,
		v_4,
		v_5,
		v_6,
		v_7,
		v_8
		\rangle :=\\
		&&\epsilon_{\beta}(v_1)
		\epsilon_{\gamma}(v_2)
		\epsilon_{\alpha+\beta}(v_3)
		\epsilon_{\beta+\gamma}(v_4)
		\epsilon_{\alpha+\beta+\gamma}(v_5)
		\epsilon_{2\beta+\gamma}(v_6)
		\epsilon_{\alpha+2\beta+\gamma}(v_7)
		\epsilon_{2\alpha+2\beta+\gamma}(v_8) \in V_\alpha
	\end{eqnarray*}
		
	The group law for $V_\alpha$ is
	\begin{eqnarray*}
		&&u * v =\\
		&&\langle
		u_1 + v_1,
		u_2 + v_2,
		u_3 + v_3,
		u_4 + v_4 + u_2 + v_1,
		u_5 + v_5 - u_3v_2,
		u_6 + v_6 + u_2v_1^2 + 2u_4v_1,\\
		&&\quad u_7 + v_7 + u_2u_3v_1 + u_2v_1v_3 + u_5v_1 + u_4v_3,
		u_8 + v_8 - u_3^2v_2 - 2u_3v_2v_3 + 2u_5v_3
		\rangle.
	\end{eqnarray*}
	We compute the action
	\begin{eqnarray*}
		\left(\begin{matrix} a & 0 \\ 0 & a^{-1} \end{matrix}\right)\cdot v
		&=&
		\langle 
		a^{-p^r}v_1,
		v_2,
		a^{p^r}v_3,
		a^{-p^r}v_4,
		a^{p^r}v_5,
		a^{-2p^r}v_6,
		v_7,
		a^{2p^r}v_8
		\rangle.
	\end{eqnarray*}
	Let $\sigma$ be in $Z^1(SL_2, V_\alpha)$ such that
	\begin{displaymath}
		\sigma\left(\left(\begin{matrix}a & 0\\0 & a^{-1}\end{matrix}\right)\right) = 0.
	\end{displaymath}
	By \ref{claim1}
	\begin{displaymath}
		\sigma\left(\left(\begin{matrix} 1 & b \\ 0 & 1 \end{matrix}\right)\right) =
		\langle 
		0,
		0,
		x_3(b),
		0,
		x_5(b),
		0,
		0,
		x_8(b)
		\rangle.
	\end{displaymath}
	Applying $\sigma$ to both sides of the identity
	\begin{displaymath}
		\left(\begin{matrix} 1 & a^2b \\ 0 & 1 \end{matrix}\right)
		= 
		\left(\begin{matrix} a & 0 \\ 0 & a^{-1} \end{matrix}\right)
		\left(\begin{matrix} 1 & b \\ 0 & 1 \end{matrix}\right)
		\left(\begin{matrix} a^{-1} & 0 \\ 0 & a \end{matrix}\right),
	\end{displaymath}
	yields
	\begin{eqnarray*}
		x_3(a^2b) &=& a^{p^r}x_3(b)\\
		x_5(a^2b) &=& a^{p^r}x_5(b)\\
		x_8(a^2b) &=& a^{2p^r}x_8(b).
	\end{eqnarray*}
	Since the polynomials $x_3, x_5, x_8$ are homomorphisms (\ref{ufixes}) we get
	\begin{eqnarray*}
		\sum_i \lambda_i (a^2b)^{p^i} &=& a^{p^r} \sum_i \lambda_i b^{p^i}\\
		\sum_i \mu_i (a^2b)^{p^i} &=& a^{p^r} \sum_i \mu_i b^{p^i}\\
		\sum_i \nu_i (a^2b)^{p^i} &=& a^{2p^r} \sum_i \nu_i b^{p^i},
	\end{eqnarray*}
	from which we can deduce
	\begin{eqnarray*}
		x_3 \neq 0 &\Longrightarrow& x_3(b) = \lambda b^{p^{r+1}}, p = 2\\
		x_5 \neq 0 &\Longrightarrow& x_5(b) = \mu b^{p^{r+1}}, p = 2\\
		x_8 \neq 0 &\Longrightarrow& x_8(b) = \nu b^{p^r}.
	\end{eqnarray*}
	Therefore, if the image of the group of upper (uni-)triangular matrices of $SL_2$ under $\sigma$ is $\langle U_{\alpha+\beta}, U_{\alpha+\beta+\gamma}, U_{2\alpha+2\beta+\gamma} \rangle$ then the characteristic of $k$ must be 2, and so the image is a product of commuting root groups.
\end{example}

[State the result]

Things to do here:

\begin{itemize}
% \item Refer to Structure/Classification Theorem to get the homomorphisms $\rho_r$
% \item Choosing $\sigma$ s.t. $\sigma\left(\begin{matrix} t&0\\0&t^{-1}\end{matrix}\right) = 0$
% \item Letting $\sigma\left(\begin{matrix}1&u\\0&1\end{matrix}\right)_i = p_i(u)$
% \item $\left(\begin{matrix}t&0\\0&t^{-1}\end{matrix}\right)
% \left(\begin{matrix}1&u\\0&1\end{matrix}\right)
% \left(\begin{matrix}t^{-1}& 0\\ 0& t\end{matrix}\right) = 
% \left(\begin{matrix}1&t^2u\\0&1\end{matrix}\right)\Rightarrow 
% \left(\begin{matrix}t&0\\0&t^{-1}\end{matrix}\right)\cdot\left(p_i(u)\right) = \left(p_i(t^2u)\right)$
% \item $\left(\begin{matrix}t&0\\0&t^{-1}\end{matrix}\right)=
% \left(\begin{matrix}1&u_1\\0&1\end{matrix}\right)
% \left(\begin{matrix}1&u_2\\0&1\end{matrix}\right)\Rightarrow
% p_i(u_1+u_2) = p_i(u_1) * \left(\begin{matrix}1&u_1\\0&1\end{matrix}\right)\cdot p_i(u_2)$. 
% Usually end up with $p_i$ homomorphisms.
% \item Know $\sigma\left(\begin{matrix} 1 & u \\ 0 & 1\end{matrix}\right)$. Calc.
% $\sigma\left(\begin{matrix}a&b\\0&a^{-1}\end{matrix}\right)
% =
% \left(\begin{matrix}a&0\\0&a^{-1}\end{matrix}\right)\cdot
% \sigma\left(\begin{matrix}1&a^{-1}c\\0&1\end{matrix}\right)$
\item Can get  $\sigma\left(\begin{matrix}d^{-1}&0\\c&d\end{matrix}\right)$ by a similar argument.
\item Calc.  $\sigma\left(\begin{matrix}0&1\\-1&0\end{matrix}\right) =
\sigma\left(
\left(\begin{matrix}1&1\\0&1\end{matrix}\right)
\left(\begin{matrix}1&0\\-1&1\end{matrix}\right)
\left(\begin{matrix}1&1\\0&1\end{matrix}\right)
\right)$
\item Compare with fact $
\left(\begin{matrix}t&0\\0&t^{-1}\end{matrix}\right)\cdot
\sigma\left(\begin{matrix}0&1\\-1&0\end{matrix}\right)=
\sigma\left(\begin{matrix}0&1\\-1&0\end{matrix}\right)$. Now we know $\sigma$ exactly on $B$ and $n_\gamma$.
\item Already know 
$\sigma\left(\begin{matrix}a&b\\c&d\end{matrix}\right)$ if $c=0$. Now calc.

$\sigma\left(\begin{matrix}a&b\\c&d\end{matrix}\right) =
\sigma\left(
\left(\begin{matrix}1&ac^{-1}\\0&1\end{matrix}\right)
\left(\begin{matrix}0&1\\-1&0\end{matrix}\right)
\left(\begin{matrix}-c&-d\\0&-c^{-1}\end{matrix}\right)
\right)$
\item We now have fact $\sigma'\in Z^1(SL_2, V) \Rightarrow \sigma'\sim\sigma$ and know the form of $\sigma$. To check ``$\Leftarrow$'' direction apply $\sigma$ to the Steinberg relations.
\item Find all $\tau\in Z^1(SL_2, V)$ conj. to $\sigma$ and also zero on $\left(\begin{matrix}t & 0 \\ 0 & t^{-1}\end{matrix}\right)$ by calculating $\tau(g) = v*\sigma(g)*g\cdot v^{-1}$.
\item Can now state conj. classes of 1-cocycles by inspection.
\item Extend classes to $P$-conjugacy by action of $Z(L)$. Explain why \ldots
\item $G$-conjugacy \ldots
\end{itemize}


\section{A rank 1 calculation}

[INCLUDE $G_2$ OR $B_2$ CALCULATIONS]

Let $T$ be a maximal torus of $B_2$ over an algebraically closed field $k$ of characteristic $p$. We label the positive roots for $B_2$ as $\alpha, \beta, \alpha + \beta, 2\alpha + \beta$. We have from [reference Humphreys 33.4]:
\begin{eqnarray*}
\epsilon_\beta (y) \epsilon_\alpha (x) &=& \epsilon_\alpha (x) \epsilon_\beta (y) \epsilon_{\alpha + \beta} (xy) \epsilon_{2\alpha+\beta} (x^2y) \\
\epsilon_{\alpha + \beta} (y) \epsilon_\alpha (x) &=& \epsilon_\alpha (x) \epsilon_{\alpha + \beta} (y) \epsilon_{2\alpha + \beta} (2xy),
\end{eqnarray*}
and 
\begin{eqnarray*}
n_\alpha \epsilon_\beta(x) n_\alpha^{-1} &=& \epsilon_{2\alpha+\beta}(x)\\
n_\alpha \epsilon_{\alpha+\beta}(x) n_\alpha^{-1} &=& \epsilon_{\alpha+\beta}(-x)\\
n_\alpha \epsilon_{2\alpha+\beta}(x) n_\alpha^{-1} &=& \epsilon_{\beta}(x)\\
n_\beta \epsilon_\alpha(x) n_\beta^{-1} &=& \epsilon_{\alpha+\beta}(x)\\
n_\beta \epsilon_{\alpha+\beta}(x) n_\beta^{-1} &=& \epsilon_{\alpha}(-x)\\
n_\beta \epsilon_{2\alpha+\beta}(x) n_\beta^{-1} &=& \epsilon_{2\alpha+\beta}(x)
\end{eqnarray*}
A proper parabolic subgroup of $B_2$ is conjugate to one of
\begin{eqnarray*}
P_\alpha &=& \langle B, U_{-\alpha} \rangle\\
P_\beta &=& \langle B, U_{-\beta} \rangle,
\end{eqnarray*}
where $B$ is the Borel subgroup of $B_2$ containing $T$
\begin{eqnarray*}
B=\langle T, U_\alpha, U_\beta, U_{\alpha + \beta}, U_{2\alpha+\beta}\rangle.
\end{eqnarray*}
The two parabolic subgroups have the Levi decompositions
\begin{eqnarray*}
P_\alpha &=& L_\alpha \ltimes R_u(P_\alpha) \\
&=& \langle T, U_\alpha, U_{-\alpha} \rangle \ltimes \langle U_\beta, U_{\alpha + \beta}, U_{2\alpha + \beta} \rangle \\
P_\beta &=& L_\beta \ltimes R_u(P_\beta) \\
&=& \langle T, U_\beta, U_{-\beta} \rangle \ltimes \langle U_\alpha, U_{\alpha+\beta}, U_{2\alpha + \beta} \rangle
\end{eqnarray*}

\subsection{Example}
Let $V$ be the unipotent radical of the parabolic subgroup of $B_2$ defined by the (short) root $\alpha$:
\begin{eqnarray*}
V=R_u(P_\alpha)=\langle U_\beta, U_{\alpha + \beta}, U_{2\alpha + \beta} \rangle,
\end{eqnarray*}
and let $\rho_r$ be the homomorphism from $SL_2 \rightarrow L_\alpha$ defined by
\begin{eqnarray*}
\rho_r\left(\begin{matrix} 1 & u \\ 0 & 1\end{matrix}\right) &=& \epsilon_\alpha(u^{p^r}) \\
\rho_r\left(\begin{matrix} t & 0 \\ 0 & t^{-1}\end{matrix}\right) &=& \alpha ^\vee(t^{p^r}) \\
\rho_r\left(\begin{matrix} 0 & 1 \\ -1 & 0\end{matrix}\right) &=& n_ \alpha,
\end{eqnarray*}
where $r$ is some non-negative integer.

Note that $V$ is abelian. Now $SL_2$ acts on $V$ via $\rho_r$: write $\mathbf{v} = \epsilon_ \beta (v_1)\epsilon_{\alpha+\beta}(v_2) \epsilon_{2\alpha+\beta}(v_3)$ in $V$ as a column vector
\begin{eqnarray*}
\mathbf{v} &=& \left(\begin{matrix} v_1 \\ v_2 \\ v_3 \end{matrix}\right),
\end{eqnarray*}
and
\begin{eqnarray*}
\left(\begin{matrix} 1 & u \\ 0 & 1\end{matrix}\right) \cdot \mathbf{v} 
&=& 
\rho_r\left(\begin{matrix} 1 & u \\ 0 & 1\end{matrix}\right) \mathbf{v}\left( \rho_r\left(\begin{matrix} 1 & u \\ 0 & 1\end{matrix}\right)\right)^{-1} \\
&=&
\epsilon_ \alpha (u^{p^r}) \epsilon_ \beta (v_1)\epsilon_{\alpha+\beta}(v_2) \epsilon_{2\alpha+\beta}(v_3) \epsilon_ \alpha (-u^{p^r}) \\
&=&
\epsilon_ \alpha (u^{p^r}) \epsilon_ \beta (v_1) \epsilon_{\alpha+\beta}(v_2) \epsilon_ \alpha (-u^{p^r}) \epsilon_{2\alpha+\beta}(v_3) \\
&=&
\epsilon_ \alpha (u^{p^r}) \epsilon_ \beta (v_1)  \epsilon_ \alpha (-u^{p^r}) \epsilon_{\alpha+\beta}(v_2) \epsilon_{2\alpha+\beta}(-2u^{p^r}v_2)\epsilon_{2\alpha+\beta}(v_3)\\
&=&
\epsilon_ \alpha (u^{p^r}) \epsilon_ \alpha (-u^{p^r})  \epsilon_ \beta (v_1) \epsilon_{\alpha+\beta}(-u^{p^r}v_1) \epsilon_{2\alpha+\beta}(u^{2p^r}v_1) \epsilon_{\alpha+\beta}(v_2) \epsilon_{2\alpha+\beta}(v_3-2u^{p^r}v_2)\\
&=&
\epsilon_ \beta (v_1)  \epsilon_{\alpha+\beta}(v_2-u^{p^r}v_1) \epsilon_{2\alpha+\beta}(v_3-2u^{p^r}v_2 + u^{2p^r}v_1)\\
&=& \left(\begin{matrix} v_1 \\ v_2-u^{p^r}v_1 \\ v_3-2u^{p^r}v_2 + u^{2p^r}v_1 \end{matrix}\right) \\
%\end{eqnarray*}
%\begin{eqnarray*}
\left(\begin{matrix} t & 0 \\ 0 & t^{-1}\end{matrix}\right) \cdot \mathbf{v} 
&=&
\rho_r\left(\begin{matrix} t & 0 \\ 0 & t^{-1}\end{matrix}\right) \mathbf{v} \left( \rho_r\left(\begin{matrix} t & 0 \\ 0 & t^{-1}\end{matrix}\right)\right)^{-1} \\
&=& 
\alpha^\vee(t^{p^r}) 
\epsilon_ \beta (v_1)
\epsilon_{\alpha+\beta}(v_2)
\epsilon_{2\alpha+\beta}(v_3) 
(\alpha^\vee(t^{p^r}))^{-1} \\
&=& 
\epsilon_\beta \left(\beta(\alpha^\vee(t^{p^r}))v_1\right)
\epsilon_{\alpha+\beta} \left((\alpha+\beta)(\alpha ^\vee(t^{p^r}))v_2 \right)
\epsilon_{2\alpha+\beta} \left((2\alpha+\beta)(\alpha ^\vee(t^{p^r}))v_3 \right)\\
&=& 
\epsilon_ \beta \left((t^{p^r})^{\langle \beta, \alpha \rangle}v_1 \right)
\epsilon_{\alpha+\beta} \left((t^{p^r})^{\langle \alpha+\beta, \alpha \rangle}v_2 \right)
\epsilon_{2\alpha+\beta} \left((t^{p^r})^{\langle 2\alpha+\beta, \alpha \rangle}v_3 \right)\\
&=& 
\left(\begin{matrix} t^{-2p^r}v_1 \\ v_2\\ t^{2p^r} v_3 \end{matrix}\right) \\
%\end{eqnarray*}
%\begin{eqnarray*}
\left(\begin{matrix} 0 & 1 \\ -1 & 0 \end{matrix}\right) \cdot \mathbf{v} 
&=&
\rho_r\left(\begin{matrix} 0 & 1 \\ -1 & 0\end{matrix}\right) \mathbf{v}\left( \rho_r\left(\begin{matrix} 0 & 1 \\ -1 & 0\end{matrix}\right)\right)^{-1} \\
&=& 
n_ \alpha  \epsilon_ \beta (v_1)\epsilon_{\alpha+\beta}(v_2) \epsilon_{2\alpha+\beta}(v_3) n_ \alpha^{-1}\\
&=& 
n_ \alpha  \epsilon_\beta (v_1) n_ \alpha^{-1}n_ \alpha \epsilon_{\alpha+\beta}(v_2) n_ \alpha^{-1} n_ \alpha \epsilon_{2\alpha+\beta}(v_3) n_ \alpha^{-1}\\
&=& 
\epsilon_{2\alpha+\beta} (v_1) \epsilon_{\alpha+\beta}(-v_2)  \epsilon_{\beta}(v_3) \\
&=& 
\epsilon_{\beta}(v_3) \epsilon_{\alpha+\beta}(-v_2) \epsilon_{2\alpha+\beta} (v_1)\\
&=& \left(\begin{matrix} v_3 \\ -v_2 \\ v_1 \end{matrix}\right).
\end{eqnarray*}

We can combine the above calculations to get an explicit formula for the action of $SL_2$ on $V$:
\begin{eqnarray*}
\left(\begin{matrix} a & b \\ c & d\end{matrix}\right) \cdot \mathbf{v} &=&
\left(\begin{matrix}
d^{2p^r}v_1 -2(cd)^{p^r}v_2 + c^{2p^r}v_3 \\
(ad + bc)^{p^r}v_2 - (bd)^{p^r}v_1 - (ac)^{p^r}v_3 \\
b^{2p^r}v_1 -2(ab)^{p^r}v_2 + a^{2p^r}v_3
\end{matrix}\right)
\end{eqnarray*}

Now let $\sigma'$ in $Z^1(SL_2, V)$ be a 1-cocycle from $SL_2\rightarrow V$. By [some reference] $\sigma'$ is conjugate to a 1-cocycle $\sigma$ that has the additional property that
\begin{eqnarray*}
\sigma\left( \begin{matrix} t & 0 \\ 0 & t^{-1}\end{matrix}\right) &=& 
\left( \begin{matrix} 0 \\ 0 \\0\end{matrix}\right),
\end{eqnarray*}
for all $t$ in $k^*$. Since we are ultimately concerned with the 1-cohomology, that is, conjugacy classes of 1-cocycles, we may proceed with $\sigma$ instead.

Since $\sigma$ is a morphism of varieties, each component of $\sigma\left(\begin{matrix} 1& u \\ 0 & 1\end{matrix}\right)$ should be a polynomial function of $u$, so let
\begin{eqnarray*}
\sigma\left(\begin{matrix} 1& u \\ 0 & 1\end{matrix}\right) &=&
\left(\begin{matrix} p_1(u) \\ p_2(u) \\ p_3(u) \end{matrix}\right).
\end{eqnarray*}

Now we make use of the very simple relations
\begin{eqnarray}
\label{eq:no1a}
\left(\begin{matrix} 1 & t^2u \\ 0 & 1\end{matrix}\right) &=&
\left(\begin{matrix} t & 0 \\ 0 & t^{-1}  \end{matrix}\right) 
\left(\begin{matrix} 1 & u \\ 0 & 1 \end{matrix}\right) 
\left(\begin{matrix}  t^{-1} & 0 \\ 0 & t \end{matrix}\right) \\
\label{eq:no2a}
\left(\begin{matrix} 1 & u_1 + u_2 \\ 0 & 1 \end{matrix}\right) &=&
\left(\begin{matrix} 1 & u_1 \\ 0 & 1 \end{matrix}\right) 
\left(\begin{matrix} 1 & u_2 \\ 0 & 1 \end{matrix}\right),
\end{eqnarray}
to get further information on the polynomials $p_i$ $(i=1,2,3)$.

If we apply $\sigma$ to both sides of (\ref{eq:no1a}), using the 1-cocycle condition on the right hand side, then we get
\begin{eqnarray*}
\sigma\left(
	\left(\begin{matrix} 1 & t^2u \\ 0 & 1\end{matrix}\right)
\right) &=&
\sigma\left(
	\left(\begin{matrix} t & 0 \\ 0 & t^{-1}  \end{matrix}\right) 
	\left(\begin{matrix} 1 & u \\ 0 & 1 \end{matrix}\right) 
	\left(\begin{matrix}  t^{-1} & 0 \\ 0 & t \end{matrix}\right)
\right) 
\\
&=&
\sigma\left(
	\left(\begin{matrix} t & 0 \\ 0 & t^{-1}  \end{matrix}\right) 
\right) +
\left(\begin{matrix} t & 0 \\ 0 & t^{-1}  \end{matrix}\right) \cdot
\sigma\left(
	\left(\begin{matrix} 1 & u \\ 0 & 1 \end{matrix}\right) 
	\left(\begin{matrix}  t^{-1} & 0 \\ 0 & t \end{matrix}\right)
\right) 
\\
&=& 
\sigma\left(
	\left(\begin{matrix} t & 0 \\ 0 & t^{-1}  \end{matrix}\right) 
\right) +
\left(\begin{matrix} t & 0 \\ 0 & t^{-1}  \end{matrix}\right) \cdot
\left(
	\sigma\left(
		\left(\begin{matrix} 1 & u \\ 0 & 1 \end{matrix}\right) 
	\right) +
	\left(\begin{matrix} 1 & u \\ 0 & 1 \end{matrix}\right) \cdot
	\sigma\left(
		\left(\begin{matrix}  t^{-1} & 0 \\ 0 & t \end{matrix}\right)
	\right)
\right)
\\
&=& 
\left(\begin{matrix} t & 0 \\ 0 & t^{-1}  \end{matrix}\right) \cdot
\sigma\left(
	\left(\begin{matrix} 1 & u \\ 0 & 1 \end{matrix}\right) 
\right).
\end{eqnarray*}
That is,
\begin{eqnarray}
\label{t:b2a_1}
p_1(t^2u) &=& t^{-2p^r} p_1(u) \\
\label{t:b2a_2}
p_2(t^2u) &=& p_2(u) \\
\label{t:b2a_3}
p_3(t^2u) &=&  t^{2p^r}p_3(u).
\end{eqnarray}

From (\ref{t:b2a_2}) it is clear that $p_2$ is constant, so there is a $\lambda$ in $k$ such that $p_2(x)=\lambda$ for all $x$ in $k$. Now notice that on the left hand side of (\ref{t:b2a_1}) there are only non-negative powers of $t$, and on the right hand side there are only non-positive powers of $t$. This equality is only satisfied if $p_1(x)=0$ for all $x$ in $k$, so $p_1$ is the zero polynomial.

We apply $\sigma$ to (\ref{eq:no2a}) and using the 1-cocycle condition to obtain
\begin{eqnarray*}
\sigma\left(
	\left(\begin{matrix} 1 & u_1 + u_2 \\ 0 & 1 \end{matrix}\right)
\right)
&=&
\sigma\left(
	\left(\begin{matrix} 1 & u_1 \\ 0 & 1 \end{matrix}\right) 
	\left(\begin{matrix} 1 & u_2 \\ 0 & 1 \end{matrix}\right)
\right) \\
&=&
\sigma\left(
	\left(\begin{matrix} 1 & u_1 \\ 0 & 1 \end{matrix}\right)
\right) +
\left(\begin{matrix} 1 & u_1 \\ 0 & 1 \end{matrix}\right) \cdot
\sigma\left(
	\left(\begin{matrix} 1 & u_2 \\ 0 & 1 \end{matrix}\right)
\right).
\end{eqnarray*}

That is,
\begin{eqnarray}
\label{u:b2a_1}
p_2(u_1 + u_2) &=& p_2(u_1) + p_2(u_2) \\
\label{u:b2a_2}
p_3(u_1 + u_2) &=& p_3(u_1) + p_3(u_2) - 2u_1^{p^r}p_2(u_2).
\end{eqnarray}


Since $p_2$ is constant, (\ref{u:b2a_1}) implies that $p_2$ is the zero polynomial, which means (\ref{u:b2a_2}) becomes
\begin{eqnarray*}
p_3(u_1 + u_2) &=& p_3(u_1) + p_3(u_2). 
\end{eqnarray*}
Hence $p_3$ is a homomorphism, that is, of the form
\begin{eqnarray}
\label{p:b2a_1}
p_3(x) &=& \sum_{i=0}^N \mu_i x^{p^i},
\end{eqnarray}
for some $u_i$ in $k$.

Now combining (\ref{t:b2a_3}) and (\ref{p:b2a_1}) yields
\begin{eqnarray}
\label{b2a1}
\sum_{i=0}^N \mu_i (t^2u)^{p^i} &=& t^{2p^r}\sum_{i=0}^N \mu_i u^{p^i}.
\end{eqnarray}

If $p_3$ is not the zero polynomial then there is a non-zero $\mu_l$ for some index $l$. By equating the coefficients of $u$ in (\ref{b2a1}) we get
\begin{eqnarray*}
\mu_lt^{2p^l} &=& \mu_lt^{2p^r} \\
\Longrightarrow\quad p^l &=& p^r.
\end{eqnarray*}
Therefore $l=r$. This means that the only non-zero $\mu_i$ is already specified by the choice of $r$ in defining $\rho_r$. 

Letting $\mu_l = \mu$ in $k$, we have
\begin{eqnarray*}
\sigma\left(
	\left(\begin{matrix} a & b \\ 0 & a^{-1}\end{matrix}\right)
\right) &=&
\sigma\left(
	\left(\begin{matrix} a & 0 \\ 0 & a^{-1}\end{matrix}\right)
	\left(\begin{matrix} 1 & a^{-1}b \\ 0 & 1\end{matrix}\right)
\right) \\
&=&
\sigma\left(
	\left(\begin{matrix} a & 0 \\ 0 & a^{-1}\end{matrix}\right)
\right) +
\left(\begin{matrix} a & 0 \\ 0 & a^{-1}\end{matrix}\right) \cdot
\sigma\left(
	\left(\begin{matrix} 1 & a^{-1}b \\ 0 & 1\end{matrix}\right)
\right) \\
&=&
\left(\begin{matrix} a & 0 \\ 0 & a^{-1}\end{matrix}\right) \cdot
\left(\begin{matrix} 0 \\ 0 \\ \mu(a^{-1}b)^{p^{r}}\end{matrix}\right) \\
&=&
\left(\begin{matrix} 0 \\ 0 \\ \mu(ab)^{p^{r}}\end{matrix}\right).
\end{eqnarray*}

If we are to find a non-trivial 1-cohomology $H^1(SL_2, V)$ then $\sigma$ cannot be a 1-coboundary. But if the characteristic of $k$, $p$, is not equal to $2$ then by setting $\mathbf{v}$ in $V$ as
\begin{eqnarray*}
\mathbf{v} &=& \left(\begin{matrix} 0 \\ \mu2^{-1} \\ 0 \end{matrix}\right),
\end{eqnarray*}

we get for all $a$ in $k^*$ and all $b$ in $k$
\begin{eqnarray*}
\chi_v\left(
\left(\begin{matrix}a & b \\ 0 & a^{-1}\end{matrix}\right)
\right) &=&
\mathbf{v}-\left(\begin{matrix}a & b \\ 0 & a^{-1}\end{matrix}\right)\cdot \mathbf{v} \\
&=&
\left(\begin{matrix} 0 \\ \mu2^{-1} \\ 0 \end{matrix}\right) - 
\left(\begin{matrix} 0 \\ \mu2^{-1}\ \\ -\mu(ab)^{p^r} \end{matrix}\right)
 \\
&=&
\left(\begin{matrix} 0 \\ 0 \\ \mu(ab)^{p^r} \end{matrix}\right) \\
&=&
\sigma\left(
\left(\begin{matrix} a & b \\ 0 & a^{-1}\end{matrix}\right)
\right).
\end{eqnarray*}
That is, $\sigma$ takes the value of a 1-coboundary on the subgroup of upper triangular matrices of $SL_2$. By [some reference], this means that $\sigma$ is a 1-coboundary from the whole of $SL_2 \rightarrow V$, and hence the 1-cohomology $H^1(SL_2, V)$ is trivial. Therefore it is necessary to proceed with $p=2$:
\begin{eqnarray}
\label{b2a:b}
\sigma\left(
	\left(\begin{matrix} a & b \\ 0 & a^{-1}\end{matrix}\right)
\right) 
&=&
\left(\begin{matrix} 0 \\ 0 \\ \mu(ab)^{2^{r}}\end{matrix}\right).
\end{eqnarray}

We can use an entirely similar argument to the one in calculating (\ref{b2a:b}) to show that
\begin{eqnarray*}
\sigma\left(
	\left(\begin{matrix} d^{-1} & 0 \\ c & d\end{matrix}\right)
\right) &=&
\left(\begin{matrix} \mu'(cd)^{2^{r}} \\ 0 \\ 0 \end{matrix}\right),
\end{eqnarray*}
for some $\mu'$ in $k$. 

We are now interested in the value of
\begin{eqnarray*}
\sigma\left(
	\left(\begin{matrix} 0 & 1 \\ -1 & 0 \end{matrix}\right)
\right) &=&
\sigma\left(
	\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right)
\right),
\end{eqnarray*}
remembering that $k$ now has characteristic 2. On the one hand
\begin{eqnarray*}
\sigma\left(
	\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right)
\right) &=&
\sigma\left(
	\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
	\left(\begin{matrix} 1 & 0 \\ 1 & 1 \end{matrix}\right)
	\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
\right) \\
&=&
\sigma\left(
	\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
\right) +
\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right) \cdot
\sigma\left(
	\left(\begin{matrix} 1 & 0 \\ 1 & 1 \end{matrix}\right)
	\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
\right) \\ 
&=&
\sigma\left(
	\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
\right) +
\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right) \cdot
\left(
	\sigma\left(
		\left(\begin{matrix} 1 & 0 \\ 1 & 1 \end{matrix}\right)
	\right) +
	\left(\begin{matrix} 1 & 0 \\ 1 & 1 \end{matrix}\right)\cdot
	\sigma\left(
		\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
	\right)
\right)\\ 
&=&
\left(\begin{matrix} 0 \\ 0 \\ \mu \end{matrix}\right)
+
\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right) \cdot
\left(
	\left(\begin{matrix} \mu' \\ 0 \\ 0 \end{matrix}\right)
	+
	\left(\begin{matrix} 1 & 0 \\ 1 & 1 \end{matrix}\right)\cdot
	\left(\begin{matrix} 0 \\ 0 \\ \mu \end{matrix}\right)
\right)\\ 
&=&
\left(\begin{matrix} 0 \\ 0 \\ \mu \end{matrix}\right)
+
\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right) \cdot
\left(
	\left(\begin{matrix} \mu' \\ 0 \\ 0 \end{matrix}\right)
	+
	\left(\begin{matrix} \mu \\ \mu \\ \mu \end{matrix}\right)
\right)\\ 
&=&
\left(\begin{matrix} 0 \\ 0 \\ \mu \end{matrix}\right)
+
\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right) \cdot
\left(\begin{matrix} \mu + \mu' \\ \mu \\ \mu \end{matrix}\right)\\
&=&
\left(\begin{matrix} 0 \\ 0 \\ \mu \end{matrix}\right)
+
\left(\begin{matrix} \mu + \mu' \\ \mu' \\ \mu' \end{matrix}\right)
\,=\,
\left(\begin{matrix} \mu + \mu' \\ \mu' \\ \mu + \mu' \end{matrix}\right).
\end{eqnarray*}

On the other hand, by applying $\sigma$ to both sides of the equality
\begin{eqnarray*}
\left(\begin{matrix} t & 0 \\ 0 & t^{-1} \end{matrix}\right)
\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right) &=&
\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right)
\left(\begin{matrix} t^{-1} & 0 \\ 0 & t \end{matrix}\right),
\end{eqnarray*} 
we get
\begin{eqnarray*}
\left(\begin{matrix} t & 0 \\ 0 & t^{-1} \end{matrix}\right) \cdot
\sigma\left(
\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right)
\right) &=&
\sigma\left(
\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right)
\right).
\end{eqnarray*}
Therefore $\sigma\left(
\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right)
\right)$ is an element of $V$ that is fixed by the action of $\left(\begin{matrix} t & 0 \\ 0 & t^{-1} \end{matrix}\right)$. Referring to the formula for the action of $SL_2$ on $V$ we see that such an element of $V$ is of the form
\begin{eqnarray*}
\left(\begin{matrix} 0 \\ * \\ 0 \end{matrix}\right),
\end{eqnarray*}
which implies that $\mu = \mu'$.

Finally, consider
\begin{eqnarray*}
\sigma\left(
\left(\begin{matrix}a & b \\ c & d\end{matrix}\right)
\right).
\end{eqnarray*}

If $c=0$ then we already have
\begin{eqnarray*}
\sigma\left(
\left(\begin{matrix}a & b \\ 0 & a^{-1}\end{matrix}\right)
\right)
&=&
\left(\begin{matrix}0 \\ 0 \\ \mu(ab)^{2^r}\end{matrix}\right).
\end{eqnarray*}
Otherwise $c^{-1}$ exists and we can compute
\begin{eqnarray*}
\sigma\left(
\left(\begin{matrix}a & b \\ c & d\end{matrix}\right)
\right) &=&
\sigma\left(
	\left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right)
	\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right)
	\left(\begin{matrix} c & d \\ 0 & c^{-1} \end{matrix}\right)
\right) \\
&=&
\sigma\left(
	\left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right)
\right) + 
\left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right)\cdot
\sigma\left(
	\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right)
	\left(\begin{matrix} c & d \\ 0 & c^{-1} \end{matrix}\right)
\right) \\
&=&
\sigma\left(
	\left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right)
\right) + 
\left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right)\cdot
\left(
	\sigma\left(
		\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right)
	\right) +
	\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right)\cdot
	\sigma\left(
		\left(\begin{matrix} c & d \\ 0 & c^{-1} \end{matrix}\right)
	\right)
\right)\\
&=&
\left(\begin{matrix} 0 \\ 0 \\ \mu(ac^{-1})^{2^r} \end{matrix}\right)
 + 
\left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right)\cdot
\left(
	\left(\begin{matrix} 0 \\ \mu \\ 0 \end{matrix}\right)
	 +
	\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right)\cdot
	\left(\begin{matrix} 0 \\ 0 \\ \mu(cd)^{2^r} \end{matrix}\right)
\right)
\\
&=&
\left(\begin{matrix} 0 \\ 0 \\ \mu(ac^{-1})^{2^r} \end{matrix}\right)
 + 
\left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right)\cdot
\left(
	\left(\begin{matrix} 0 \\ \mu \\ 0 \end{matrix}\right)
	 +
	\left(\begin{matrix} \mu(cd)^{2^r} \\ 0 \\ 0 \end{matrix}\right)
\right)\\
&=&
\left(\begin{matrix} 0 \\ 0 \\ \mu(ac^{-1})^{2^r} \end{matrix}\right)
 + 
\left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right)\cdot
\left(\begin{matrix} \mu(cd)^{2^r} \\ \mu \\ 0 \end{matrix}\right)\\
&=&
\left(\begin{matrix} 0 \\ 0 \\ \mu(ac^{-1})^{2^r} \end{matrix}\right)
 + 
\left(\begin{matrix} \mu(cd)^{2^r} \\ \mu + (ac^{-1})^{2^r} \mu(cd)^{2^r} \\  (ac^{-1})^{2^{r+1}} \mu(cd)^{2^r} \end{matrix}\right)\\
&=&
\left(\begin{matrix} \mu(cd)^{2^r} \\ \mu(1 + ad)^{2^r} \\ \mu(ac^{-1})^{2^r}(1 + ad)^{2^r} \end{matrix}\right)
\,=\,\left(\begin{matrix} \mu(cd)^{2^r} \\ \mu(bc)^{2^r} \\ \mu(ab)^{2^r} \end{matrix}\right).
\end{eqnarray*}

In fact, we see that
\begin{eqnarray*}
\sigma\left(
\left(\begin{matrix}a & b \\ c & d\end{matrix}\right)
\right) &=&
\left(\begin{matrix} \mu(cd)^{2^r} \\ \mu(bc)^{2^r} \\ \mu(ab)^{2^r} \end{matrix}\right),
\end{eqnarray*}
holds in either case.

\emph{[Show converse - Steinberg relations]}

Now if $\sigma$ is in the same conjugacy class as $\tau$ then by [some reference]
\begin{eqnarray*}
\tau\left(
\left(\begin{matrix} a & b \\ c & d\end{matrix}\right)
\right) &=&
\mathbf{v} +
\sigma\left(
\left(\begin{matrix} a & b \\ c & d\end{matrix}\right)
\right) 
+\left(\begin{matrix} a & b \\ c & d\end{matrix}\right)\cdot \mathbf{v}.
\end{eqnarray*}

As before, we consider 1-cocycles that are zero on $\left(\begin{matrix} t & 0 \\ 0 & t^{-1}\end{matrix}\right)$, so this means considering $\mathbf{v}$ that is fixed by the action of $\left(\begin{matrix} t & 0 \\ 0 & t^{-1}\end{matrix}\right)$:

\begin{eqnarray*}
\tau\left(
\left(\begin{matrix} a & b \\ c & d\end{matrix}\right)
\right) 
&=&
\left(\begin{matrix} 0 \\ v_2 \\ 0 \end{matrix}\right) +
\sigma\left(
\left(\begin{matrix} a & b \\ c & d\end{matrix}\right)
\right) 
+\left(\begin{matrix} a & b \\ c & d\end{matrix}\right)\cdot \left(\begin{matrix} 0 \\ v_2 \\ 0 \end{matrix}\right) \\
&=&
\left(\begin{matrix} 0 \\ v_2 \\ 0 \end{matrix}\right) +
\left(\begin{matrix} \mu(cd)^{2^r} \\ \mu(bc)^{2^r} \\ \mu(ab)^{2^r} \end{matrix}\right)
+\left(\begin{matrix} 0 \\ v_2 \\ 0 \end{matrix}\right) \\
&=&\left(\begin{matrix}  \mu(cd)^{2^r} \\ \mu(bc)^{2^r} \\ \mu(ab)^{2^r} \end{matrix}\right).
\end{eqnarray*}

Therefore each $\mu$ in $k$ corresponds to a conjugacy class of 1-cocycles $[\sigma_\mu]$ from $SL_2\rightarrow V$ where
\begin{eqnarray*}
\sigma_\mu\left(\left(\begin{matrix}a & b \\ c & d \end{matrix}\right)\right) &=&
\left(\begin{matrix}  \mu(cd)^{2^r} \\ \mu(bc)^{2^r} \\ \mu(ab)^{2^r} \end{matrix}\right),
\end{eqnarray*}
and the 1-cocycle $\tau$ is in the class $[\sigma_\mu]$ if there is a $\mathbf{v}$ in $V$ such that
\begin{eqnarray*}
\tau\left(\left(\begin{matrix}a & b \\ c & d \end{matrix}\right)\right) &=&
\mathbf{v} + \sigma_\mu\left(\left(\begin{matrix}a & b \\ c & d \end{matrix}\right)\right)
+ \left(\begin{matrix}a & b \\ c & d \end{matrix}\right)\cdot\mathbf{v}.
\end{eqnarray*}

As discussed in [ref previous section] we can use this result to find the 1-cocycles from $SL_2 \rightarrow P_\alpha$ by considering the action of $Z(L_\alpha)^\circ$, the connected centre of the Levi subgroup $L_\alpha$. Now, 
\begin{eqnarray*}
Z(L_\alpha)^\circ &=& \langle \gamma^\vee(x)\,|\,x \in k \rangle
\end{eqnarray*}
where $\gamma$ is a root in $\Phi_{\alpha, \beta}$ such that
\begin{eqnarray}
\langle \alpha, \gamma \rangle &=& 0.
\end{eqnarray}
Since $\gamma = m\alpha + n\beta$ for some integers $m,n$, we have
\begin{eqnarray}
\langle \alpha, \gamma \rangle &=& \langle \alpha, m\alpha + n\beta \rangle
\end{eqnarray}
and so
\begin{eqnarray*}
 \langle \alpha, m\alpha + n\beta \rangle &=& 0 \\
\Longleftrightarrow \quad  \langle m\alpha + n\beta, \alpha \rangle &=& 0 \\
\Longleftrightarrow \quad  m\langle \alpha, \alpha \rangle + n \langle \beta, \alpha\rangle &=& 0 \\
\Longleftrightarrow \quad  2m - 2n &=& 0 \\
\Longleftrightarrow \quad  m &=& n.
\end{eqnarray*}
Therefore $Z(L_ \alpha)^\circ = \langle (\alpha + \beta)^\vee(x)\,|\,x \in k \rangle$. Taking an element $\mathbf{s} = (\alpha + \beta)^\vee(s)$ of $Z(L_\alpha)^\circ$ we compute the action of $\mathbf{s}$ on the 1-cocycle $\sigma_\mu$ as follows:
\begin{eqnarray*}
\left(\mathbf{s}\cdot \sigma_\mu\right)
\left(\begin{matrix} a & b \\ c & d\end{matrix} \right) 
&=&
(\alpha + \beta)^\vee(s) \epsilon_\beta \left(\mu (cd)^{2^r} \right)\epsilon_{\alpha+\beta} \left(\mu(bc)^{2^r} \right)\epsilon_{2\alpha + \beta} \left(\mu(ab)^{2^r} \right)(\alpha + \beta)^\vee(s)^{-1}\\
&=&  \epsilon_\beta\left(s^{\langle\beta , \alpha+\beta\rangle}\mu (cd)^{2^r} \right)\epsilon_{\alpha+\beta} \left(s^{\langle \alpha+\beta, \alpha+\beta \rangle} \mu(bc)^{2^r} \right)\epsilon_{2\alpha + \beta} \left(s^{\langle 2\alpha+\beta, \alpha+\beta\rangle}\mu(ab)^{2^r}\right) \\
&=&
\left(\begin{matrix}
(s^2\mu)(cd)^{2^r} \\
(s^2\mu)(bc)^{2^r} \\
(s^2\mu)(ab)^{2^r}
\end{matrix}\right).
\end{eqnarray*}

So we see that the infinitely many conjugacy classes of 1-cocycles from $SL_2\rightarrow V$ collapse to just two classes when we consider the action of $Z(L_\alpha)^\circ$, that is, moving from $V$-conjugacy to $P_\alpha$-conjugacy:
\begin{eqnarray*}
\left[ \sigma_0 \right] &=& \left\{ \sigma_0 \right\} \\
\left[ \sigma_1 \right] &=& \left\{ \sigma_\mu \, |\, \mu \in k^* \right\}.
\end{eqnarray*}

\subsection{Example}
Let $V$ be the unipotent radical of the parabolic subgroup of $B_2$ defined by the (long) root $\beta$:
\begin{eqnarray*}
V=R_u(P_\beta)=\langle U_\alpha, U_{\alpha + \beta}, U_{2\alpha + \beta} \rangle,
\end{eqnarray*}
and let $\rho_r$ be the homomorphism from $SL_2 \rightarrow L_\beta$ defined by
\begin{eqnarray*}
\rho_r\left(\begin{matrix} 1 & u \\ 0 & 1\end{matrix}\right) &=& \epsilon_\beta(u^{p^r}) \\
\rho_r\left(\begin{matrix} t & 0 \\ 0 & t^{-1}\end{matrix}\right) &=& \beta^\vee(t^{p^r}) \\
\rho_r\left(\begin{matrix} 0 & 1 \\ -1 & 0\end{matrix}\right) &=& n_\beta,
\end{eqnarray*}
where $r$ is some non-negative integer.

Note that $V$ is not abelian in general. The Group Law for $V$ can be computed as follows. Let $\mathbf{v}, \mathbf{w}$ in $V$. We have, using notation similar to the previous example
\begin{eqnarray*}
\mathbf{v}*\mathbf{w}
&=& 
\epsilon_\alpha(v_1)\epsilon_{\alpha+\beta}(v_2)\epsilon_{2\alpha+\beta}(v_3) \epsilon_\alpha(w_1)\epsilon_{\alpha+\beta}(w_2)\epsilon_{2\alpha+\beta}(w_3)\\
&=& 
\epsilon_\alpha(v_1)\epsilon_{\alpha+\beta}(v_2) \epsilon_\alpha(w_1)\epsilon_{\alpha+\beta}(w_2)\epsilon_{2\alpha+\beta}(v_3)\epsilon_{2\alpha+\beta}(w_3)\\
&=& 
\epsilon_\alpha(v_1) \epsilon_\alpha(w_1) \epsilon_{\alpha + \beta}(v_2)\epsilon_{2\alpha+\beta}(2v_2w_1)\epsilon_{\alpha+\beta}(w_2)\epsilon_{2\alpha+\beta}(v_3)\epsilon_{2\alpha+\beta}(w_3)\\
&=& 
\epsilon_\alpha(v_1 + w_1) \epsilon_{\alpha + \beta}(v_2 + w_2)\epsilon_{2\alpha+\beta}(v_3 + w_3 + 2v_2w_1)\\
&=&
\left(\begin{matrix}
v_1 + w_1 \\
v_2 + w_2 \\
v_3 + w_3 + 2v_2w_1
\end{matrix}\right).
\end{eqnarray*}

Now we compute the action of $SL_2$ on $V$ via $\rho_r$. Let $\mathbf{v}$ be an element of $V$:
\begin{eqnarray*}
\left(\begin{matrix} 1 & u \\ 0 & 1\end{matrix}\right) \cdot \mathbf{v} &=& \rho_r\left(\begin{matrix} 1 & u \\ 0 & 1\end{matrix}\right) \mathbf{v}\left( \rho_r\left(\begin{matrix} 1 & u \\ 0 & 1\end{matrix}\right)\right)^{-1} \\
&=&\epsilon_\beta (u^{p^r}) \epsilon_\alpha (v_1)\epsilon_{\alpha+\beta}(v_2) \epsilon_{2\alpha+\beta}(v_3) \epsilon_\beta (-u^{p^r}) \\
&=&\epsilon_\alpha (v_1) \epsilon_\beta (u^{p^r}) \epsilon_{\alpha+\beta}(u^{p^r}v_1) \epsilon_{2\alpha+\beta}(u^{p^r}v_1^2) \epsilon_{\alpha+\beta}(v_2) \epsilon_{2\alpha+\beta}(v_3) \epsilon_\beta (-u^{p^r})  \\
&=&\epsilon_\alpha (v_1) \epsilon_\beta (u^{p^r}) \epsilon_{\alpha+\beta}(v_2 + u^{p^r}v_1) \epsilon_{2\alpha+\beta}(v_3 + u^{p^r}v_1^2)  \epsilon_\beta (-u^{p^r})  \\
&=&\epsilon_\alpha (v_1) \epsilon_{\alpha+\beta}(u^{p^r}v_1) \epsilon_{2\alpha+\beta}(u^{p^r}v_1^2) \epsilon_{\alpha+\beta}(v_2) \epsilon_{2\alpha+\beta}(v_3)\epsilon_\beta (u^{p^r})  \epsilon_\beta (-u^{p^r})  \\
&=&\epsilon_\alpha (v_1)  \epsilon_{\alpha+\beta}(v_2 + u^{p^r}v_1) \epsilon_{2\alpha+\beta}(v_3 + u^{p^r}v_1^2) \\
&=& \left(\begin{matrix} v_1 \\ v_2 + u^{p^r}v_1\\ v_3 + u^{p^r}v_1^2 \end{matrix}\right)\\
%\end{eqnarray*}
%\begin{eqnarray*}
\left(\begin{matrix} t & 0 \\ 0 & t^{-1}\end{matrix}\right) \cdot \mathbf{v} &=&
\rho_r\left(\begin{matrix} t & 0 \\ 0 & t^{-1}\end{matrix}\right) \mathbf{v}\left( \rho_r\left(\begin{matrix} t & 0 \\ 0 & t^{-1}\end{matrix}\right)\right)^{-1} \\
&=& \beta^\vee(t^{p^r}) \epsilon_\alpha (v_1)
\epsilon_{\alpha+\beta}(v_2)
\epsilon_{2\alpha+\beta}(v_3) (\beta^\vee(t^{p^r}))^{-1} \\
&=& \epsilon_\alpha \left(\alpha(\beta^\vee(t^{p^r}))v_1\right)
\epsilon_{\alpha+\beta} \left((\alpha+\beta)(\beta^\vee(t^{p^r}))v_2 \right)
\epsilon_{2\alpha+\beta} \left((2\alpha+\beta)(\beta^\vee(t^{p^r}))v_3 \right)\\
&=& \epsilon_\alpha \left((t^{p^r})^{\langle \alpha, \beta \rangle}v_1 \right)
\epsilon_{\alpha+\beta} \left((t^{p^r})^{\langle \alpha+\beta, \beta \rangle}v_2 \right)
\epsilon_{2\alpha+\beta} \left((t^{p^r})^{\langle 2\alpha+\beta, \beta \rangle}v_3 \right)\\
&=& \left(\begin{matrix} t^{-p^r}v_1 \\ t^{p^r}v_2\\ v_3 \end{matrix}\right) \\
%\end{eqnarray*}
%\begin{eqnarray*}
\left(\begin{matrix} 0 & 1 \\ -1 & 0 \end{matrix}\right) \cdot \mathbf{v} &=&
\rho_r\left(\begin{matrix} 0 & 1 \\ -1 & 0\end{matrix}\right) \mathbf{v}\left( \rho_r\left(\begin{matrix} 0 & 1 \\ -1 & 0\end{matrix}\right)\right)^{-1} \\
&=& n_\beta  \epsilon_\alpha (v_1)\epsilon_{\alpha+\beta}(v_2) \epsilon_{2\alpha+\beta}(v_3) n_\beta^{-1}\\
&=& n_\beta  \epsilon_\alpha (v_1) n_\beta^{-1}n_\beta \epsilon_{\alpha+\beta}(v_2) n_\beta^{-1} n_\beta \epsilon_{2\alpha+\beta}(v_3) n_\beta^{-1}\\
&=& \epsilon_{\alpha+\beta} (v_1) \epsilon_{\alpha}(-v_2)  \epsilon_{2\alpha+\beta}(v_3) \\
&=&\epsilon_{\alpha}(-v_2)  \epsilon_{\alpha+\beta} (v_1)  \epsilon_{2\alpha+\beta}(v_3 - 2v_1v_2) \\
&=& \left(\begin{matrix} -v_2 \\ v_1 \\ v_3 - 2v_1v_2 \end{matrix}\right).
\end{eqnarray*}
Or, more explicitly
\begin{eqnarray*}
\left(\begin{matrix}
a & b \\ c & d
\end{matrix} \right) \cdot \mathbf{v} &=&
\left(\begin{matrix}
c^{p^r}v_2 + d^{p^r}v_1 \\
a^{p^r}v_2 + b^{p^r}v_1 \\
v_3 + (ac)^{p^r}v_2^2 + (bd)^{p^r}v_1^2 + 2(bc)^{p^r}v_1v_2
\end{matrix}\right).
\end{eqnarray*}

As in the previous example we let $\sigma$ in $Z^1(SL_2, V)$ be a 1-cocycle from $SL_2\rightarrow V$ such that
\begin{eqnarray*}
\sigma\left( \begin{matrix} t & 0 \\ 0 & t^{-1}\end{matrix}\right) &=& 
\left( \begin{matrix} 0 \\ 0 \\0\end{matrix}\right),
\end{eqnarray*}
for all $t$ in $k^*$, and
\begin{eqnarray*}
\sigma\left(\begin{matrix} 1& u \\ 0 & 1\end{matrix}\right) &=&
\left(\begin{matrix} p_1(u) \\ p_2(u) \\ p_3(u) \end{matrix}\right),
\end{eqnarray*}
for all $u$ in $k$.

We use the same two identities to further investigate the 1-cocycle:
\begin{eqnarray}
\label{eq:no1}
\left(\begin{matrix} 1 & t^2u \\ 0 & 1\end{matrix}\right) &=&
\left(\begin{matrix} t & 0 \\ 0 & t^{-1}  \end{matrix}\right) 
\left(\begin{matrix} 1 & u \\ 0 & 1 \end{matrix}\right) 
\left(\begin{matrix}  t^{-1} & 0 \\ 0 & t \end{matrix}\right) \\
\label{eq:no2}
\left(\begin{matrix} 1 & u_1 + u_2 \\ 0 & 1 \end{matrix}\right) &=&
\left(\begin{matrix} 1 & u_1 \\ 0 & 1 \end{matrix}\right) 
\left(\begin{matrix} 1 & u_2 \\ 0 & 1 \end{matrix}\right),
\end{eqnarray}

Applying $\sigma$ to both sides of (\ref{eq:no1}), using the 1-cocycle condition on the right hand side, we get
\begin{eqnarray*}
\sigma\left(
	\left(\begin{matrix} 1 & t^2u \\ 0 & 1\end{matrix}\right)
\right) &=&
\left(\begin{matrix} t & 0 \\ 0 & t^{-1}  \end{matrix}\right) \cdot
\sigma\left(
	\left(\begin{matrix} 1 & u \\ 0 & 1 \end{matrix}\right) 
\right).
\end{eqnarray*}
That is
\begin{eqnarray}
\label{t:b2ab_1}
p_1(t^2u) &=& t^{-p^r} p_1(u) \\
\label{t:b2ab_2}
p_2(t^2u) &=& t^{p^r}p_2(u) \\
\label{t:b2ab_3}
p_3(t^2u) &=&  p_3(u).
\end{eqnarray}

From (\ref{t:b2ab_3}) we find that $p_3$ is constant-valued, say $p_3(x)=\lambda$ in $k$ for all $x$ in $k$. From (\ref{t:b2ab_1}) we see that there are only non-negative powers of $t$ on the left hand side and only non-positive powers the right hand side. Therefore $p_1$ is the zero polynomial.

Now applying $\sigma$ to both sides of (\ref{eq:no2}):
\begin{eqnarray*}
\sigma\left(
	\left(\begin{matrix} 1 & u_1 + u_2 \\ 0 & 1 \end{matrix}\right)
\right)
&=&
\sigma\left(
	\left(\begin{matrix} 1 & u_1 \\ 0 & 1 \end{matrix}\right) 
	\left(\begin{matrix} 1 & u_2 \\ 0 & 1 \end{matrix}\right)
\right) \\
&=&
\sigma\left(
	\left(\begin{matrix} 1 & u_1 \\ 0 & 1 \end{matrix}\right)
\right) *
\left(\begin{matrix} 1 & u_1 \\ 0 & 1 \end{matrix}\right) \cdot
\sigma\left(
	\left(\begin{matrix} 1 & u_2 \\ 0 & 1 \end{matrix}\right)
\right) \\
&=&
\left(\begin{matrix} 0 \\ p_2(u_1) \\ \lambda \end{matrix}\right)
*
\left(\begin{matrix} 1 & u_1 \\ 0 & 1 \end{matrix}\right) \cdot
\left(\begin{matrix} 0 \\ p_2(u_2) \\ \lambda \end{matrix}\right) \\
&=&
\left(\begin{matrix} 0 \\ p_2(u_1) \\ \lambda \end{matrix}\right)
*
\left(\begin{matrix} 0 \\ p_2(u_2) \\ \lambda \end{matrix}\right) \\
&=&
\left(\begin{matrix} 0 \\ p_2(u_1) + p_2(u_2)\\ 2\lambda \end{matrix}\right)
\end{eqnarray*}

That is,
\begin{eqnarray}
\label{u:b2ab_1}
p_2(u_1 + u_2) &=& p_2(u_1) + p_2(u_2) \\
\label{u:b2ab_2}
\lambda &=& 2\lambda.
\end{eqnarray}

By (\ref{u:b2ab_2}) we see that $p_3$ is in fact the zero polynomial, and (\ref{u:b2ab_1}) implies that $p_2$ is a homomorphism, that is, of the form
\begin{eqnarray}
\label{p:b2ab_1}
p_2(x) &=& \sum_{i=0}^N \mu_i x^{p^i},
\end{eqnarray}
for some $\mu_i$ in $k$.

Now combining (\ref{t:b2ab_2}) and (\ref{p:b2ab_1}) yields
\begin{eqnarray}
\label{b2ab1}
\sum_{i=0}^N \mu_i (t^2u)^{p^i} &=& t^{p^r}\sum_{i=0}^N \mu_i u^{p^i}.
\end{eqnarray}

If $p_2$ is not the zero polynomial then there is a non-zero $\mu_l$ for some index $l$. By equating coefficients of $u^{p^i}$ in (\ref{b2ab1}) we get
\begin{eqnarray*}
\mu_lt^{2p^l} &=& \mu_lt^{p^r} \\
\Longrightarrow\quad 2p^l &=& p^r.
\end{eqnarray*}
Thus 2 divides $p^r$, and since $p$ is a prime, $p=2$. Furthermore $l=r-1$. This means that the non-zero $\mu_l$ is already specified by the choice of $r$ in defining $\rho_r$, and that $r$ must be non-zero if $p_2$ is to be non-zero.

Referring to the Group Law we see that $V$ is abelian in characteristic 2, so we will use the `+' symbol for combining elements of $V$ from now on.

Proceeding with $p=2$, $r>0$ and letting $\mu_l = \mu$, we have
\begin{eqnarray*}
\sigma\left(
	\left(\begin{matrix} a & b \\ 0 & a^{-1}\end{matrix}\right)
\right) &=&
\sigma\left(
	\left(\begin{matrix} a & 0 \\ 0 & a^{-1}\end{matrix}\right)
	\left(\begin{matrix} 1 & a^{-1}b \\ 0 & 1\end{matrix}\right)
\right) \\
&=&
\sigma\left(
	\left(\begin{matrix} a & 0 \\ 0 & a^{-1}\end{matrix}\right)
\right) +
\left(\begin{matrix} a & 0 \\ 0 & a^{-1}\end{matrix}\right) \cdot
\sigma\left(
	\left(\begin{matrix} 1 & a^{-1}b \\ 0 & 1\end{matrix}\right)
\right) \\
&=&
\left(\begin{matrix} a & 0 \\ 0 & a^{-1}\end{matrix}\right) \cdot
\left(\begin{matrix} 0 \\ \mu(a^{-1}b)^{2^{r-1}} \\ 0 \end{matrix}\right) \\
&=&
\left(\begin{matrix} 0 \\ \mu(ab)^{2^{r-1}} \\ 0 \end{matrix}\right).
\end{eqnarray*}

We can use an entirely similar argument to show that
\begin{eqnarray*}
\sigma\left(
	\left(\begin{matrix} d^{-1} & 0 \\ c & d\end{matrix}\right)
\right) &=&
\left(\begin{matrix} \mu'(cd)^{2^{r-1}} \\ 0 \\ 0 \end{matrix}\right),
\end{eqnarray*}
for some $\mu'$ in $k$. 

We are now interested in the value of
\begin{eqnarray*}
\sigma\left(
	\left(\begin{matrix} 0 & 1 \\ -1 & 0 \end{matrix}\right)
\right) &=&
\sigma\left(
	\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right)
\right).
\end{eqnarray*}

We have
\begin{eqnarray*}
\sigma\left(
	\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right)
\right) &=&
\sigma\left(
	\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
	\left(\begin{matrix} 1 & 0 \\ 1 & 1 \end{matrix}\right)
	\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
\right) \\
&=&
\sigma\left(
	\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
\right) +
\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right) \cdot
\sigma\left(
	\left(\begin{matrix} 1 & 0 \\ 1 & 1 \end{matrix}\right)
	\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
\right) \\ 
&=&
\sigma\left(
	\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
\right) +
\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right) \cdot
\left(
	\sigma\left(
		\left(\begin{matrix} 1 & 0 \\ 1 & 1 \end{matrix}\right)
	\right) +
	\left(\begin{matrix} 1 & 0 \\ 1 & 1 \end{matrix}\right)\cdot
	\sigma\left(
		\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
	\right)
\right)\\ 
&=&
\left(\begin{matrix} 0 \\ \mu \\ 0 \end{matrix}\right)
+
\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right) \cdot
\left(
	\left(\begin{matrix} \mu' \\ 0 \\ 0 \end{matrix}\right)
	+
	\left(\begin{matrix} 1 & 0 \\ 1 & 1 \end{matrix}\right)\cdot
	\left(\begin{matrix} 0 \\ \mu \\ 0 \end{matrix}\right)
\right)\\ 
&=&
\left(\begin{matrix} 0 \\ \mu \\ 0 \end{matrix}\right)
+
\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right) \cdot
\left(
	\left(\begin{matrix} \mu' \\ 0 \\ 0 \end{matrix}\right)
	+
	\left(\begin{matrix} \mu \\ \mu \\ \mu^2 \end{matrix}\right)
\right)\\ 
&=&
\left(\begin{matrix} 0 \\ \mu \\ 0 \end{matrix}\right)
+
\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right) \cdot
\left(\begin{matrix} \mu' + \mu \\ \mu \\ \mu^2 \end{matrix}\right)\\
&=&
\left(\begin{matrix} 0 \\ \mu \\ 0 \end{matrix}\right)
+
\left(\begin{matrix} \mu' + \mu \\ \mu' \\ \mu'^2 \end{matrix}\right)\\
&=&
\left(\begin{matrix} \mu' + \mu \\ \mu' + \mu \\ \mu'^2 \end{matrix}\right).
\end{eqnarray*}

Since $\sigma\left(\left(\begin{matrix} 0 & 1 \\ 1 & 0\end{matrix}\right)\right)$ is fixed under the action of $\left(\begin{matrix} t & 0 \\ 0 & t^{-1}\end{matrix}\right)$ for all $t$ in $k^*$ we must have $\mu'=\mu$. 

Suppose $c\neq0$. We have
\begin{eqnarray*}
\sigma\left(\left(\begin{matrix} a & b \\ c & d \end{matrix}\right)\right) 
&=&
\sigma\left(
	\left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right)
	\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right)
	\left(\begin{matrix} c & d \\ 0 & c^{-1} \end{matrix}\right)
\right) \\
&=&
\sigma\left(
	\left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right)
\right) +
\left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right) \cdot
\left(
	\sigma\left(
		\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right)
	\right) +
	\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right) \cdot
	\sigma\left(
		\left(\begin{matrix} c & d \\ 0 & c^{-1} \end{matrix}\right)
	\right)
\right) \\
&=&
\left(\begin{matrix} 0 \\ \mu(ac^{-1})^{2^{r-1}} \\ 0 \end{matrix}\right)
+
\left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right) \cdot
\left(
	\left(\begin{matrix} 0 \\ 0 \\ \mu^2 \end{matrix}\right)
	+
	\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right) \cdot
	\left(\begin{matrix} 0 \\ \mu(cd)^{2^{r-1}} \\ 0 \end{matrix}\right)
\right) \\
&=&
\left(\begin{matrix} 0 \\ \mu(ac^{-1})^{2^{r-1}} \\ 0 \end{matrix}\right)
+
\left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right) \cdot
\left(
	\left(\begin{matrix} 0 \\ 0 \\ \mu^2 \end{matrix}\right)
	+
	\left(\begin{matrix} \mu(cd)^{2^{r-1}} \\ 0 \\ 0 \end{matrix}\right)
\right) \\
&=&
\left(\begin{matrix} 0 \\ \mu(ac^{-1})^{2^{r-1}} \\ 0 \end{matrix}\right)
+
\left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right) \cdot
\left(\begin{matrix} \mu(cd)^{2^{r-1}}\\ 0 \\ \mu^2 \end{matrix}\right) \\
&=&
\left(\begin{matrix} 0 \\ \mu(ac^{-1})^{2^{r-1}} \\ 0 \end{matrix}\right)
+
\left(\begin{matrix} \mu(cd)^{2^{r-1}} \\ (ac^{-1})^{2^r} \mu(cd)^{2^{r-1}}  \\ \mu^2 +  (ac^{-1})^{p^r} \left(\mu(cd)^{2^{r-1}}\right)^2  \end{matrix}\right)
 \\
&=&
\left(\begin{matrix}  \mu(cd)^{2^{r-1}}  \\ \mu\left(ac^{-1} + a^2c^{-1}d \right)^{2^{r-1}} \\ \mu^2\left( 1 + ad\right)^{2^r} \end{matrix}\right)  \\
&=&
\left(\begin{matrix}  \mu(cd)^{2^{r-1}}  \\ \mu\left(ab \right)^{2^{r-1}} \\ \mu^2\left( bc \right)^{2^r} \end{matrix}\right). 
\end{eqnarray*}

But the above result holds when $c=0$ too, so we conclude that
\begin{eqnarray*}
\sigma\left(\left(\begin{matrix} a & b \\ c & d \end{matrix}\right)\right) &=&
\left(\begin{matrix}  \mu(cd)^{2^{r-1}}  \\ \mu\left(ab \right)^{2^{r-1}} \\ \mu^2\left( bc \right)^{2^r} \end{matrix}\right). 
\end{eqnarray*}

\emph{[Show converse is true]}

As in the previous example, we choose a $\mathbf{v}$ in $V$ that is fixed by $\left(\begin{matrix} t & 0 \\ 0 & t^{-1}\end{matrix}\right)$ and compute
\begin{eqnarray*}
\tau\left(\left(\begin{matrix} a & b \\ c & d \end{matrix}\right)\right) &=&
\mathbf{v} + \sigma\left(\left(\begin{matrix} a & b \\ c & d \end{matrix}\right)\right) + 
\left(\begin{matrix} a & b \\ c & d \end{matrix}\right) \cdot \mathbf{v} \\
 &=&
\left(\begin{matrix} 0 \\ 0 \\ v_3 \end{matrix}\right) + 
\left(\begin{matrix}  \mu(cd)^{2^{r-1}}  \\ \mu\left(ab \right)^{2^{r-1}} \\ \mu^2\left( bc \right)^{2^r} \end{matrix}\right)+ 
\left(\begin{matrix} a & b \\ c & d \end{matrix}\right) \cdot 
\left(\begin{matrix} 0 \\ 0 \\ v_3 \end{matrix}\right) \\
 &=&
\left(\begin{matrix} 0 \\ 0 \\ v_3 \end{matrix}\right) + 
\left(\begin{matrix}  \mu(cd)^{2^{r-1}}  \\ \mu\left(ab \right)^{2^{r-1}} \\ \mu^2\left( bc \right)^{2^r} \end{matrix}\right)+ 
\left(\begin{matrix} 0 \\ 0 \\ v_3 \end{matrix}\right) \\
&=& \left(\begin{matrix}  \mu(cd)^{2^{r-1}}  \\ \mu\left(ab \right)^{2^{r-1}} \\ \mu^2\left( bc \right)^{2^r} \end{matrix}\right),
\end{eqnarray*}
which tells us that for each $\mu$ in $k$ we get a distinct conjugacy class of 1-cocycles $[\sigma_\mu]$ from $SL_2 \rightarrow V$, where
\begin{eqnarray*}
\sigma_\mu\left(\left(\begin{matrix} a & b \\ c & d \end{matrix}\right)\right) &=&
\left(\begin{matrix}  \mu(cd)^{2^{r-1}}  \\ \mu\left(ab \right)^{2^{r-1}} \\ \mu^2\left( bc \right)^{2^r} \end{matrix}\right).
\end{eqnarray*}

But as before if we consider the action of $Z(L_\beta)$ on our 1-cocycles
\begin{eqnarray*}
(\mathbf{s}\cdot \sigma_\mu)\left(\begin{matrix} a & b \\ c & d \end{matrix}\right) &=&
(2\alpha + \beta)^\vee(s) \cdot \sigma_\mu\left(\left(\begin{matrix} a & b \\ c & d \end{matrix}\right)\right)\\
&=&
\left(\begin{matrix}  (s\mu)(cd)^{2^{r-1}}  \\ (s\mu)\left(ab \right)^{2^{r-1}} \\ (s\mu)^2\left( bc \right)^{2^r} \end{matrix}\right).
\end{eqnarray*}
our infinitely many $V$-conjugacy classes collapse to just two $P_\beta$-conjugacy classes:
\begin{eqnarray*}
\left[\sigma_0\right] &=& \left\{ \sigma_0 \right\}, \\
\left[\sigma_1\right] &=& \left\{ \sigma_\mu \, | \, \mu \in k^* \right\}.
\end{eqnarray*}

\section{A rank 2 calculation}

Is $Im(\rho_{r,s})$ irred in $L_{\gamma,\delta}$?

No $\rightarrow$ $Im(\rho_{r,s})$ inside (a conjugate of) $P_\gamma(B_2)$ or $P_\delta(B2)$.
Then it's inside  $P_\gamma = L_\gamma \ltimes R_u(P_\gamma)$ or  $P_\delta = L_\delta \ltimes R_u(P_\delta)$, so it's inside $L_\gamma$ or $L_\delta$.

1) Know about non G-cr in $B_2$, can I put them in an $A_1A_1$?

1a) Can this sit inside a rank 1 Levi?

2) Use $B_2=SO_5$.

3) Take $Im(\rho_{r,s})$, can we conjugate it into $P_\gamma$ or $P_\delta$?

Let $\mathrm{char}(k)=2$ and set $V:=\langle U_\phi\, |\, \phi \in \Phi^+, \phi \neq \gamma + \delta,\phi \neq \gamma+2\delta\} \rangle$. We will write
%\begin{eqnarray*}
$
\mathbf{v} = \epsilon_\alpha(v_1) \epsilon_\beta(v_2) \epsilon_{\alpha+\beta}(v_3) \epsilon_{\beta+\gamma}(v_4) \epsilon_{\alpha+\beta+\gamma}(v_5) \epsilon_{\beta+\gamma+\delta}(v_6) \epsilon_{\alpha+\beta+\gamma+\delta}(v_7) \epsilon_{\beta+\gamma+2\delta}(v_8) \epsilon_{\alpha+\beta+\gamma+2\delta}(v_9) \\
\epsilon_{\beta+2\gamma+2\delta}(v_{10}) \epsilon_{\alpha+\beta+2\gamma+2\delta}(v_{11}) \epsilon_{\alpha+2\beta+2\gamma+2\delta}(v_{12}) \in V
%\end{eqnarray*}
$
as a column vector:
\begin{eqnarray*}
\mathbf{v} = \left( \begin{matrix}
	         v_1 \\
	         v_2 \\
	         v_3 \\
	         v_4 \\
	         v_5 \\
	         v_6 \\
	         v_7 \\
	         v_8 \\
	         v_9 \\
	         v_{10} \\
	         v_{11} \\
	         v_{12} 
	      \end{matrix}\right). 
\end{eqnarray*}
The Group Law on $V$ is
\begin{eqnarray*}
	     \mathbf{u}*
	     \mathbf{v}&=&
	     \mathbf{u} + \mathbf{v} +
	     \left( \begin{matrix}
	         0 \\
	         0 \\
	         u_2v_1\\
	         0 \\
	         u_4v_1 \\
	         0 \\
	         u_6v_1\\
	         0 \\
	         u_8v_1\\
	         0 \\
	         u_{10}v_1\\
	         u_{10}v_1v_2 + u_8v_1v_4 + u_6^2v_1 + u_{11}v_2 + u_{10}v_3 + u_9v_4 + u_8v_5
	         	\end{matrix}\right).
\end{eqnarray*}

For integers $r,s\geq 0$ we have a homomorphism $\rho_{r,s}:SL_2\rightarrow \widetilde{A}_1\widetilde{A}_1 < L_{\{\gamma,\delta\}}$ defined by
\begin{eqnarray*}
\rho_{r,s}   \left(\begin{matrix} % or pmatrix or bmatrix or Bmatrix or ...
      1 & u \\
      0 & 1 \\
   \end{matrix}\right) &=& \epsilon_\delta(u^{2^r})\cdot\epsilon_{\gamma+\delta}(u^{2^s}) \\
\rho_{r,s}   \left(\begin{matrix} % or pmatrix or bmatrix or Bmatrix or ...
      t & 0 \\
      0 & t^{-1} \\
   \end{matrix}\right) &=& \delta^\vee(t^{2^r})\cdot(\gamma+\delta)^\vee(t^{2^s}) \\
\rho_{r,s}   \left(\begin{matrix} % or pmatrix or bmatrix or Bmatrix or ...
      0 & 1 \\
      1 & 0 \\
   \end{matrix}\right) &=& n_\delta\cdot n_{\gamma+\delta} 
\end{eqnarray*}
from which we obtain an action of $SL_2$ on $V$:
%\begin{eqnarray*}
%	  \left( \begin{matrix}
%	      1 & u \\
%	      0 & 1 \\
%	   \end{matrix}\right) \cdot
%\mathbf{v}\, =\,
%	     \left( \begin{matrix}
%	         v_1 \\
%	         v_2 \\
%	         v_3 \\
%	         v_4 \\
%	         v_5 \\
%	         v_6 + u^{2^r}v_4 + u^{2^s}v_2 \\
%	         v_7 + u^{2^r}v_5 + u^{2^s}v_3 \\
%	         v_8 + u^{2^{r+1}}v_4 \\
%	         v_9 + u^{2^{r+1}}v_5 \\
%	         v_{10} + u^{2^{s+1}}v_2 \\
%	         v_{11} + u^{2^{r+1}}v_3 \\
%	         v_{12} + u^{2^{r+1}}v_4v_5 + u^{2^{s+1}}v_2v_3 
% \end{matrix}\right),\quad
%%	      \end{eqnarray*}\begin{eqnarray*}
%	  \left( \begin{matrix}
%	      t & 0 \\
%	      0 & t^{-1} \\
%	   \end{matrix}\right) \cdot
%\mathbf{v}\, =\,
%	     \left( \begin{matrix}
%	         v_1 \\
%	         t^{-2^{s+1}}v_2 \\
%	         t^{-2^{s+1}}v_3 \\
%	         t^{-2^{r+1}}v_4 \\
%	         t^{-2^{r+1}}v_5 \\
%	         v_6 \\
%	         v_7 \\
%	         t^{2^{r+1}}v_8 \\
%	         t^{2^{r+1}}v_9 \\
%	         t^{2^{s+1}}v_{10} \\
%	         t^{2^{s+1}}v_{11} \\
%	         v_{12} 
%	      \end{matrix}\right),
%	      \end{eqnarray*}\begin{eqnarray*}
%	  \left( \begin{matrix}
%	      0 & 1 \\
%	      1 & 0 \\
%	   \end{matrix}\right) \cdot
%\mathbf{v} &=&
%	     \left( \begin{matrix}
%	         v_1 \\
%	         v_{10} \\
%	         v_{11} \\
%	         v_8 \\
%	         v_9 \\
%	         v_6 \\
%	         v_7 \\
%	         v_4 \\
%	         v_5 \\
%	         v_2 \\
%	         v_3 \\
%	         v_{12} + v_2v_{11} + v_3v_{10} + v_4v_9 + v_5v_8        
%	      \end{matrix}\right).
%\end{eqnarray*}
\begin{eqnarray*}
\left( \begin{matrix}
	      a & b \\
	      c & d \\
	   \end{matrix}\right) \cdot \mathbf{v} &=&
	   \left( \begin{matrix}
	   v_1 \\
	   c^{2^{s+1}} v_{10} + d^{2^{s+1}}v_2 \\
	   c^{2^{s+1}} v_{11} + d^{2^{s+1}}v_3 \\
	   c^{2^{r+1}} v_{8} + d^{2^{r+1}}v_4 \\
	   c^{2^{r+1}} v_{9} + d^{2^{r+1}}v_5 \\
	   v_6 + (bd)^{2^r}v_4 + (bd)^{2^s}v_2 + (ac)^{2^r}v_8 + (ac)^{2^s}v_{10} \\
	   v_7 + (bd)^{2^r}v_5 + (bd)^{2^s}v_3 + (ac)^{2^r}v_9 + (ac)^{2^s}v_{11} \\
	   a^{2^{r+1}}v_8 + b^{2^{r+1}}v_4 \\
	   a^{2^{r+1}}v_9 + b^{2^{r+1}}v_5 \\
	   a^{2^{s+1}}v_{10} + b^{2^{s+1}}v_2 \\
	   a^{2^{s+1}}v_{11} + b^{2^{s+1}}v_3 \\
	   v_{12} + (bd)^{2^{r+1}}v_4v_5 + (bd)^{2^{s+1}}v_2v_3 + (bc)^{2^{r+1}}(v_4v_9 + v_5v_8)\\ +\, (bc)^{2^{s+1}}(v_2v_{11} + v_3v_{10}) + (ac)^{2^{r+1}}(v_8v_9) + (ac)^{2^{s+1}}(v_{10}v_{11})
	   \end{matrix} \right)
\end{eqnarray*}

Now let $\sigma$ be a 1-cocycle from $SL_2$ to $V$ such that for all $t$ in $k^*$
\begin{eqnarray*}
\sigma\left(\begin{matrix} % or pmatrix or bmatrix or Bmatrix or ...
      t & 0 \\
      0 & t^{-1} \\
   \end{matrix}\right) = \left( \begin{matrix} 0 \\ \vdots \\ 0 \end{matrix}\right).
\end{eqnarray*}
Since $\sigma$ is a morphism of varieties, each component of $\sigma\left(\begin{matrix} 1 & u \\ 0 & 1\end{matrix}\right)$ should be a polynomial function of $u$, so we let
\begin{eqnarray*}
\sigma \left( \begin{matrix} % or pmatrix or bmatrix or Bmatrix or ...
      1 & u \\
      0 & 1 \\
   \end{matrix}\right) = \left( \begin{matrix} p_1(u) \\ \vdots \\ p_{12}(u) \end{matrix} \right),
\end{eqnarray*}
where each $p_i$ ($1\leq i \leq 12$) is as required. Applying $\sigma$ to the identity
\begin{eqnarray*}
  \left( \begin{matrix}
      t & 0 \\
      0 & t^{-1} \\
   \end{matrix}\right)
   \left(\begin{matrix}
      1 & u \\
      0 & 1 \\
   \end{matrix}\right)
   \left(\begin{matrix}
      t^{-1} & 0 \\
      0 & t \\
   \end{matrix}\right) &=&
\left(   \begin{matrix}
      1 & t^2u \\
      0 & 1 \\
   \end{matrix}\right),
 \end{eqnarray*}
 gives rise to the following equations
 \begin{eqnarray}
 \label{tAct}
 p_i(t^2u) &=& \left\{    \begin{array}{ll}
       p_i(u), & i = 1,6,7,12 \\
       t^{-2^{r+1}}p_i(u), & i = 4,5 \\
       t^{-2^{s+1}}p_i(u), & i = 2,3 \\
       t^{2^{r+1}}p_i(u), & i = 8,9 \\
       t^{2^{s+1}}p_i(u), & i = 10,11 \\
    \end{array}
 \right.
 \end{eqnarray}
It is clear that for $i = 1,6,7,12$ the polynomials $p_i$ must be constant-valued, say $\lambda_i$ for some fixed $\lambda_i$ in $k$ (resp). Furthermore, since $p_i(t^2u)$ involves only non-negative powers of $t$, $p_i$ must be the zero polynomial for $i=2,3,4,5$. Now consider the identity
\begin{eqnarray*}
  \left( \begin{matrix}
      1 & u_1 \\
      0 & 1 \\
   \end{matrix}\right)
   \left(\begin{matrix}
      1 & u_2 \\
      0 & 1 \\
   \end{matrix}\right) &=&
    \left(\begin{matrix}
      1 & u_1 + u_2 \\
      0 & 1 \\
   \end{matrix}\right).
\end{eqnarray*}
Applying $\sigma$ to both sides yields
\begin{eqnarray*}
p_1(u_1 + u_2) &=& p_1(u_1) + p_1(u_2) \\
p_6(u_1 + u_2) &=& p_6(u_1) + p_6(u_2) \\
p_7(u_1 + u_2) &=& p_7(u_1) + p_7(u_2) + p_6(u_1)p_1(u_2)\\
p_8(u_1 + u_2) &=& p_8(u_1) + p_8(u_2) \\
p_9(u_1 + u_2) &=& p_9(u_1) + p_9(u_2) + p_8(u_1)p_1(u_2)\\
p_{10}(u_1 + u_2) &=& p_{10}(u_1) + p_{10}(u_2)\\
p_{11}(u_1 + u_2) &=& p_{11}(u_1) + p_{11}(u_2) + p_{10}(u_1)p_1(u_2)\\
p_{12}(u_1 + u_2) &=& p_{12}(u_1) + p_{12}(u_2) + \left(p_6(u_1)\right)^2p_1(u_2).
\end{eqnarray*}
Now we see that the constant polynomials $p_1,p_6,p_7,p_{12}$ must in fact be the zero polynomial and the remaining polynomials must be homomorphisms from $k\rightarrow k$. That is for some $w_j, x_j, y_j, z_j$ in $k$ and all $u$ in $k$
\begin{eqnarray*}
p_8(u) &=& \sum_{j=0}^N w_j u^{2^j} \\
p_9(u) &=& \sum_{j=0}^N x_j u^{2^j} \\
p_{10}(u) &=& \sum_{j=0}^N y_j u^{2^j} \\
p_{11}(u) &=& \sum_{j=0}^N z_j u^{2^j}, 
\end{eqnarray*}
If $\sigma$ is not the trivial 1-cocycle then one of the polynomials above is not the zero polynomial. Suppose for instance that $p_8$ is not the zero polynomial, so that $w_l\neq 0$ for some index $l\geq 0$. By (\ref{tAct})
\begin{eqnarray*}
\sum_{j=0}^N w_j (t^2u)^{2^j} &=& t^{2^{r+1}}\sum_{j=0}^N w_j u^{2^j} \\
\Rightarrow\quad w_l (t^2u)^{2^l} &=& t^{2^{r+1}} w_l u^{2^l} \\
\Rightarrow\quad l &=& r.
\end{eqnarray*}
The same kind of calculation for the other polynomials shows that
\begin{eqnarray*}
p_8(u) = wu^{2^{r}}, \quad p_9(u) = xu^{2^{r}}, \\
p_{10}(u) = yu^{2^{s}}, \quad p_{11}(u) = zu^{2^{s}},
\end{eqnarray*}
for some $w,x,y,z$ in $k$.

So, we have
\begin{eqnarray*}
\sigma\left(\begin{matrix} a & b \\ 0 & a^{-1} \end{matrix}\right) &=&
\sigma\left(\begin{matrix} a & 0 \\ 0 & a^{-1} \end{matrix}\right) * 
\left(\begin{matrix} a & 0 \\ 0 & a^{-1} \end{matrix}\right)\cdot
\sigma\left(\begin{matrix} 1 & a^{-1}b \\ 0 & 1 \end{matrix}\right) \\
&=&
\left( \begin{matrix}
0 \\
0 \\
0 \\
0 \\
0 \\
0 \\
0 \\
w(ab)^{2^{r+1}} \\
x(ab)^{2^{r+1}} \\
y(ab)^{2^{s+1}} \\
z(ab)^{2^{s+1}} \\
0
\end{matrix} \right).
\end{eqnarray*}

We apply the same argument using the fact that each component of $\sigma\left(\begin{matrix} 1 & 0 \\ u & 1\end{matrix}\right)$ is a polynomial function, say $p'_i(u)$ for all $u$ in $k$, to get
\begin{eqnarray*}
\sigma\left(\begin{matrix} d^{-1} & 0 \\ c & d \end{matrix}\right) &=&
\left( \begin{matrix}
0 \\
y'(cd)^{2^s} \\
z'(cd)^{2^s} \\
w'(cd)^{2^r} \\
x'(cd)^{2^r} \\
0 \\
0 \\
0 \\
0 \\
0 \\
0 \\
0
\end{matrix} \right),
\end{eqnarray*}
for some $w', x', y', z'$ in $k$.

From this we deduce that
\begin{eqnarray*}
\sigma\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right) &=&
\sigma\left(
\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
\left(\begin{matrix} 1 & 0 \\ 1 & 1 \end{matrix}\right)
\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
\right) \\
&=&
\sigma
\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
*
\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
\cdot
\sigma\left(
\left(\begin{matrix} 1 & 0 \\ 1 & 1 \end{matrix}\right)
\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
\right) \\
&=&
\sigma
\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
*
\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
\cdot
\left(
\sigma\left(\begin{matrix} 1 & 0 \\ 1 & 1 \end{matrix}\right)
*
\left(\begin{matrix} 1 & 0 \\ 1 & 1 \end{matrix}\right)
\cdot
\sigma\left(\begin{matrix} 1 & 1 \\ 0 & 1 \end{matrix}\right)
\right) \\
&=&
\left(\begin{matrix}
0 \\
y + y' \\
z + z' \\
w + w' \\
x + x' \\
w' + y' \\
x' + z' \\
w + w' \\
x + x' \\
y + y' \\
z + z' \\
w'x' + y'z'
\end{matrix}\right).
\end{eqnarray*}

Furthermore, since $\sigma\left(\begin{matrix} 0 & 1 \\ 1 & 0\end{matrix}\right)$ is fixed under the action of $\left(\begin{matrix} t & 0 \\ 0 & t^{-1}\end{matrix}\right)$, we have
\begin{eqnarray*}
\sigma\left(\begin{matrix} 0 & 1 \\ 1 & 0\end{matrix}\right) &=&
\left(\begin{matrix}
n_1 \\
0 \\
0 \\
0 \\
0 \\
n_6 \\
n_7 \\
0 \\
0 \\
0 \\
0 \\
n_{12}
\end{matrix}\right),
\end{eqnarray*} 
for some $n_1, n_6, n_7, n_{12}$ in $k$. So in fact
\begin{eqnarray*}
w' &=& w \\
x' &=& x \\
y' &=& y \\
z' &=& z \\
n_1 &=& 0\\
n_6 &=&w+y\\
n_7 &=& x+z\\
n_{12} &=& wx + yz.
\end{eqnarray*}

Consider
%\begin{eqnarray*}
$\sigma\left(\begin{matrix} a & b \\ c & d \end{matrix}\right)$.
%\end{eqnarray*}
If $c=0$ then we already have
\begin{eqnarray*}
\sigma\left(\begin{matrix} a & b \\ 0 & a^{-1} \end{matrix}\right) &=&
\sigma\left(\begin{matrix} a & 0 \\ 0 & a^{-1} \end{matrix}\right) * 
\left(\begin{matrix} a & 0 \\ 0 & a^{-1} \end{matrix}\right)\cdot
\sigma\left(\begin{matrix} 1 & a^{-1}b \\ 0 & 1 \end{matrix}\right) \\
&=&
\left( \begin{matrix}
0 \\
0 \\
0 \\
0 \\
0 \\
0 \\
0 \\
w(ab)^{2^{r+1}} \\
x(ab)^{2^{r+1}} \\
y(ab)^{2^{s+1}} \\
z(ab)^{2^{s+1}} \\
0
\end{matrix} \right).
\end{eqnarray*}
Otherwise, $c\neq 0$ and we can write
\begin{eqnarray*}
\left(\begin{matrix} a & b \\ c & d \end{matrix}\right) &=& 
\left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right)
\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right)
\left(\begin{matrix} c & d \\ 0 & c^{-1} \end{matrix}\right),
\end{eqnarray*}
and so
\begin{eqnarray*}
\sigma\left(\begin{matrix} a & b \\ c & d \end{matrix}\right) &=& 
\sigma\left(
\left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right)
\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right)
\left(\begin{matrix} c & d \\ 0 & c^{-1} \end{matrix}\right)
\right) \\
&=&
\sigma \left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right) *
\left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right) \cdot
\sigma \left( 
\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right)
\left(\begin{matrix} c & d \\ 0 & c^{-1} \end{matrix}\right)
\right) \\
&=&
\sigma \left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right) *
\left(\begin{matrix} 1 & ac^{-1} \\ 0 & 1 \end{matrix}\right) \cdot
\left( 
\sigma \left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right) *
\left(\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right) \cdot
\sigma\left(\begin{matrix} c & d \\ 0 & c^{-1} \end{matrix}\right)
\right) \\
&=&
\left(\begin{matrix}
0 \\
y(cd)^{2^s} \\
z(cd)^{2^s} \\
w(cd)^{2^r} \\
x(cd)^{2^r} \\
n_6 + w(ad)^{2^r} + y(ad)^{2^s} \\
n_7 + x(ad)^{2^r} + z(ad)^{2^s} \\
w(ab)^{2^r} \\
x(ab)^{2^r}  \\
y(ab)^{2^s} \\
z(ab)^{2^r} \\
n_{12} +wx(ad)^{2^{r+1}} + yz(ad)^{2^{s+1}}
\end{matrix}\right) \\
&=&
\left(\begin{matrix}
0 \\
y(cd)^{2^s} \\
z(cd)^{2^s} \\
w(cd)^{2^r} \\
x(cd)^{2^r} \\
w(bc)^{2^r} + y(bc)^{2^s} \\
x(bc)^{2^r} + z(bc)^{2^s} \\
w(ab)^{2^r} \\
x(ab)^{2^r}  \\
y(ab)^{2^s} \\
z(ab)^{2^r} \\
wx(bc)^{2^{r+1}} + yz(bc)^{2^{s+1}}
\end{matrix}\right).
\end{eqnarray*}
We see that in any case
\begin{eqnarray*}
\sigma\left(\begin{matrix} a & b \\ c & d \end{matrix} \right) &=& 
\left(\begin{matrix}
0 \\
y(cd)^{2^s} \\
z(cd)^{2^s} \\
w(cd)^{2^r} \\
x(cd)^{2^r} \\
w(bc)^{2^r} + y(bc)^{2^s} \\
x(bc)^{2^r} + z(bc)^{2^s} \\
w(ab)^{2^r} \\
x(ab)^{2^r}  \\
y(ab)^{2^s} \\
z(ab)^{2^r} \\
wx(bc)^{2^{r+1}} + yz(bc)^{2^{s+1}}
\end{matrix}\right).
\end{eqnarray*}

Conversely, suppose we have a map $\sigma:SL_2 \rightarrow V$ of the form
\begin{eqnarray*}
\sigma\left(\begin{matrix} a & b \\ c & d \end{matrix} \right) &=& 
\left(\begin{matrix}
0 \\
y(cd)^{2^s} \\
z(cd)^{2^s} \\
w(cd)^{2^r} \\
x(cd)^{2^r} \\
w(bc)^{2^r} + y(bc)^{2^s} \\
x(bc)^{2^r} + z(bc)^{2^s} \\
w(ab)^{2^r} \\
x(ab)^{2^r}  \\
y(ab)^{2^s} \\
z(ab)^{2^r} \\
wx(bc)^{2^{r+1}} + yz(bc)^{2^{s+1}}
\end{matrix}\right),
\end{eqnarray*}
for some $w,x,y,z$ in $k$ and integers $r,s\geq 0$. 

\emph{[Show $\sigma$ is a 1-cocycle]}

Next we shall describe $H^1(SL_2, V)$. Recall that a 1-cocycle $\tau'$ is in the same conjugacy class as $\sigma$ if there is a $\mathbf{v}$ in $V$ such that
\begin{eqnarray*}
\tau'(g) &=& \mathbf{v}*\sigma(g)*g.\mathbf{v}^{-1}
\end{eqnarray*}
for all $g$ in $SL_2$. Furthermore, $\tau'$ is conjugate to some 1-cocycle $\tau$, where $\tau$ has the added property that
\begin{eqnarray*}
\tau\left(\begin{matrix}t & 0 \\ 0 & t^{-1}\end{matrix}\right) &=& \left(\begin{matrix} 0 \\ \vdots \\ 0\end{matrix}\right).
\end{eqnarray*}
Thus $\sigma$ is conjugate to $\tau$ by some $\mathbf{v}$ in $V$ that is fixed under the action of $\left(\begin{matrix}t & 0 \\ 0 & t^{-1}\end{matrix}\right)$:
\begin{eqnarray*}
\tau\left(\begin{matrix} a & b \\ c & d \end{matrix}\right) &=&
\mathbf{v}*\sigma\left(\begin{matrix} a & b \\ c & d \end{matrix}\right) *
\left(\begin{matrix} a & b \\ c & d \end{matrix}\right) \cdot \mathbf{v}^{-1}\\
&=&
\left(\begin{matrix} 
v_1  \\
0 \\
0 \\
0 \\
0 \\
v_6 \\
v_7 \\
0 \\
0 \\
0 \\
0 \\
v_{12}
\end{matrix}\right)
*\sigma\left(\begin{matrix} a & b \\ c & d \end{matrix}\right) *
\left(\begin{matrix} a & b \\ c & d \end{matrix}\right) \cdot 
\left(\begin{matrix} 
v_1  \\
0 \\
0 \\
0 \\
0 \\
v_6 \\
v_7 + v_1v_6 \\
0 \\
0 \\
0 \\
0 \\
v_{12} + v_1v_6^2
\end{matrix}\right)\\
&=&
\left(\begin{matrix}
0 \\
y(cd)^{2^s} \\
(z+yv_1)(cd)^{2^s} \\
w(cd)^{2^r} \\
(x+wv_1)(cd)^{2^r} \\
w(bc)^{2^r} + y(bc)^{2^s} \\
(x+wv_1)(bc)^{2^r} + (z+yv_1)(bc)^{2^s} \\
w(ab)^{2^r} \\
(x+wv_1)(ab)^{2^r}  \\
y(ab)^{2^s} \\
(z+yv_1)(ab)^{2^r} \\
w(x+wv_1)(bc)^{2^{r+1}} + y(z+yv_1)(bc)^{2^{s+1}}
\end{matrix}\right)
\end{eqnarray*}
We can denote this relationship by
\begin{eqnarray*}
(w,x,y,z) &\sim& (w, x+\lambda w, y, z + \lambda y),
\end{eqnarray*}
where the 4-tuple $(w,x,y,z)$ represents the 1-cocycle 
\begin{eqnarray*}
\left(\begin{matrix} a & b \\ c & d \end{matrix}\right) &\mapsto&
\left(\begin{matrix}
0 \\
y(cd)^{2^s} \\
z(cd)^{2^s} \\
w(cd)^{2^r} \\
x(cd)^{2^r} \\
w(bc)^{2^r} + y(bc)^{2^s} \\
x(bc)^{2^r} + z(bc)^{2^s} \\
w(ab)^{2^r} \\
x(ab)^{2^r}  \\
y(ab)^{2^s} \\
z(ab)^{2^r} \\
wx(bc)^{2^{r+1}} + yz(bc)^{2^{s+1}}
\end{matrix}\right).
\end{eqnarray*}

We find infinitely many conjugacy classes, for instance for each $x, z$ in $k$ the family of classes of the form
\begin{eqnarray*}
[(0,x,0,z)] = \left\{(0,x,0,z)\right\}.
\end{eqnarray*}

Now we consider $P$-conjugacy. An element $\mathbf{s} = \alpha^\vee(s)(\beta + \gamma + \delta)^\vee(t)\in Z(L)$ acts on the 1-cocycle $\sigma$ by
\begin{eqnarray*}
(\mathbf{s}\cdot\sigma)\left(\begin{matrix} a & b \\ c & d\end{matrix}\right)
&=&
\left(\begin{matrix}
0 \\
s^{-1}t^{2}y(cd)^{2^s} \\
sz(cd)^{2^s} \\
s^{-1}t^{2}w(cd)^{2^r} \\
sx(cd)^{2^r} \\
s^{-1}t^{2}(w(bc)^{2^r} + y(bc)^{2^s}) \\
sx(bc)^{2^r} + z(bc)^{2^s} \\
s^{-1}t^{2}w(ab)^{2^r} \\
sx(ab)^{2^r}  \\
s^{-1}t^{2}y(ab)^{2^s} \\
sz(ab)^{2^r} \\
t^2(wx(bc)^{2^{r+1}} + yz(bc)^{2^{s+1}})
\end{matrix}\right)
\end{eqnarray*}






